%------------------------------%
\selectlanguage{english}
\chapter{Background on resource awareness}
\label{chp:background_resource_awareness}
\markboth{Background on resource awareness}{Chapter1}
%------------------------------%

\coolphrase {Hey, look at this}{Inti Gonzalez-Herrera}

\section{Resource Aware Programming} \label{sec:resource-awareness}

Efficient use of computational resources is an essential concern in software systems because it can reduce the costs of the infrastructure needed to execute applications.
Resource management is nevertheless particularly challenging when many stakeholders share a platform.
As a consequence, a traditional approach for resource management in applications is to relay on the runtime environment (e.g., resource management is traditionally the main role of operating systems) and use its general interface to access resources.
Although there are many advantages associated to this approach, it is known that applications built upon such general interfaces often show relative poor performance \cite{engler1995exokernel}.
The problem arises because the implementation of a general purpose abstraction must cover many use-cases.
This results in complex execution paths where few assumptions regarding how the abstraction will be used can be made.
It has been shown that the performance of many resource intensive systems can be improved by carefully specializing resource management at the application level \cite{engler1995exokernel,Belay:2014:IPD:2685048.2685053,Marinos:2014:NSS:2619239.2626311}.

The principle of bypassing a generic implementation in favor of a specialized one has been widely applied in computer science and software engineering \cite{engler1995exokernel, Munro1996,Dragos:2009:CGT:1565824.1565830, muller,Marinos:2014:NSS:2619239.2626311}.
Since resource usage is a key concern for any software system, specializing resource management is potentially beneficial for many applications.
Along this thesis, we use the term \textit{resource-aware} to refer to those applications/systems that observe, carefully manage and are aware of computational resources in order to improve their performance. 
At first, this definition might look too broad, but in reality most applications limit themselves to carefully use the resource allocations facilities offered by runtime environments.
Take for example a web server that uses a pool of threads to attend requests.
By using this pool the server is in fact carefully managing resource, but with this feature alone it is not able to observe whether the pool's size should be decreased/increased.
The key issue in the definition used in this thesis is that three elements must be present in an application/system in order to be classified as \textit{resource-aware}: observation, management and behavior modification. 
Finally, our understanding of the idea of resource-aware applications/systems is in fact closely related, but not limited,  to that of the Monitor-Analyze-Plan-Execute Loop (MAPE) \cite{Brun:2009:ESS:1573856.1573860}.
Indeed, if we consider the monitoring part as observing the resource availability and consumption, then it is simple to see how the rest of the MAPE Loop can be considered resource management and behavior modification.

Applying resource-aware techniques to develop a software system can be motivated by the need to satisfy functional or non-functional requirements.
Among these requirements we find the following:

\begin{itemize}
\item \textbf{Improve performance.}
Considerable amount of work showing the potential advantages of specializing resource management to enhance system performance have been published.
Of particular interest are use-cases that show how to reduce the execution time \cite{Polo:2011:RAS:2414338.2414352}, improve the response time \cite{no se} or increase the number of requests a web server is able to handle \cite{engler1995exokernel,Belay:2014:IPD:2685048.2685053}.

\item \textbf{Guarantee certain Quality of Service (QoS).} 
Often, when the quality of a service is evaluated, we consider properties that are related to the resources allocated to execute the service or the mechanisms used to manage resources.
For instance, misbehaviors in a property such as response time can be associated to a low resource availability \cite{uno aquji,Chechik-2009}.
Likewise, poor QoS in multimedia systems is associated to complex resource management mechanisms in the operating system kernel \cite{Black1997}.
Finally, resource-aware networking can be used to improve QoS properties such as data availability \cite{Boldrini:2008:CRA:1549824.1550106} and P2P video streaming \cite{Pianese:2007:RLA:1326320.1326323,Alhaisoni:2010:RTO:1664767.1664770}.

\item \textbf{Support per-customer resource quotas.}
In Software as a Service (SaaS), it is necessary to guarantee per-user quotas.
Although using a separated instance of the application for each user, along system-level capabilities to manage resources as we discuss in section ~\ref{}, is a solution; this approach leads to excessive resource consumption.
Instead, the trend is to design multi-tenant applications that are able to schedule the incoming requests in order to guarantee per-user quotas while sharing as most part of the application as possible \cite{KrSpAhKo2014_CCGrid_ResourceIsolation,KrWeKo2013-icwe-MTBenchmark}.    

\item \textbf{Ensure resource isolation for critical applications.}
Strong isolation among applications is often required when critical applications \cite{Knight:2002:SCS:581339.581406} share a platform with untrusted software systems.
In this context, resource usage isolation is an important concern because one application can make a second application crash by simply monopolizing the computational resources.
In this scenario, application containment \cite{Kamp00jails:confining,Soltesz:2007:COS:1272998.1273025,Madhavapeddy:2015:JJS:2789770.2789809} is a useful mechanism to support resource-aware isolation.
Providing specialized application containment requires extensive runtime environment support, based on approaches such as \textit{unikernels} \cite{Madhavapeddy:2013:ULO:2499368.2451167,Kivity:2014:OVO:2643634.2643642}, that has been under heavy development after the widespread adoption of distributed and cloud services.  

\end{itemize}

To satisfy these requirements applications often rely on specific features offered by the runtime environment or platform (e.g., operating systems, virtual machines).
In this thesis, three features that result useful to support the resource-aware programming paradigm are identified:

\begin{itemize}
\item \textbf{Resource consumption monitoring.}
Having information regarding how an application is consuming resources is mandatory to support any form of decision making that involves the modification of applications due to resource-related issues. 
In this regard, it is useful to collect data about global application consumption as well as collecting such data for each application's module with clearly defined boundaries (e.g., components).

\item \textbf{Resource reservation.}
Ensuring resource availability for specific applications or subsystems is a way to support critical applications or other systems which exhibit, for instance, timing constrains.
Reserving resources does not always imply that resources must be exclusively assigned to a process.
Instead, for certain resources such as CPU time it is possible to allocate the requested resource once it is needed.

\item \textbf{Observing resource availability (overcommitment).} 
Applications that are aware of extra resource availability are able to temporarily use such resources to improve the QoS they provide.
In addition, applications can nicely modify their behavior to collaborate with other systems that share a platform if they are aware of their consumption, the resource availability and also of what other applications demand.
\end{itemize}

Along this work, we focus on two of these features: \textit{resource consumption monitoring} and \textit{resource reservation}.
In particular, this thesis devotes a significant amount of space discussing how to deal with the problem of efficiently monitoring the quantity of resources consumed by different parts of an application.

\todo{Fill the box}
\extracomment{Resource-aware vs Real-Time requirments}{

This is an easy way to box text within a document!

czxcvzc. fsfdsfsd.
}

Providing support for the mentioned features highly depend on the target abstractions provided by the runtime environment.
In an operating systems resource consumption monitoring is often provided at per-process basis while
virtual machine monitors (VMMs) tend to offer per virtual machine resource management.
In this thesis, we focus on offering support for resource-aware programming in managed runtime environments.
Hence, the next section presents a brief overview of the main properties of these systems. 

\begin{comment}

Debe resaltarse en un parrafo inicial como todo lo que hace una computadora esta en definitiva asociado con recursos.

Esta seccion describe y define lo que entendemos por resource aware programming. Esto no es muy sencillo puesto que no hay una definion muy formal que digamos.

De hecho, este es un termino al que se hace referencia en varios lugares para referirse a la capacidad de ciertos sistemas de software para adaptar su funconamiento en base ala existencia de recursos.

Es valido destacar en este punto,  la relacion que tiene esto con los sistemas adaptables y reconfigutrables y en defintiva con el concepto general de MAPE Loop.

Ademas, se persiguen objetivos como los siguientes cuando se quiere ser resource aware:
\begin{itemize}
\item Mejorar eficiencia
\item Mejorar parametros de calidad de la aplicacion como availability
\item Asegurar resource isolation para critical applications
\item Garantizar cuotas de uso para distintios usuarios
\end{itemize}

Como lo vemos, hay tres cualidades que pueden pedirse cuando se quiere ser resource aware:
\begin{itemize}
\item Resource consumption monitoring
\item Resource reservation
\item Observing resource availability (overcommitment) 
\end{itemize}




Un punto escencial aqui es establecer bien claro cual es la diferencia entre:
\begin{itemize}
\item resource awareness vs real-time
\item resource reservation vs scheduling
\end{itemize}

El objetivo de esta seccion es dar una fundamentacion clara sobre las motivaciones que guian esta tesis y mostrar los temas en los que estara enfocada.

Es una seccion general que sin embargo tendra citas lo mas actualizadas posible en las secciones cables. La longitud estimada es de 2 a 3 paginas.

\end{comment}

\section{Managed Runtime Environments} \label{sec:mrtes}

Applications written using a given programming language often execute in a runtime environment which implements every single built-in feature of the language and supports a specific instruction set.
For a language such as \textit{C++} the runtime environment is in charge, among others, of supporting the \textit{throw/exception} mechanisms and the \textit{type casting} mechanism.
It is common to ship the runtime environment along the application when applications are deployed as native code because external dependencies are avoided in such a way.
However, developers that use mature languages such as \textit{C++} or \textit{Object Pascal} lack useful built-in features that both can ease the development process, and improve applications' quality.

In 1995 the first mainstream managed runtime environment (MRTE), Java, was created. It became a success even if it was not the first language providing built-in features such as automatic memory management, dynamic loading of portable code, or support for the object-oriented paradigm due to three main reasons: the level of maturity of many technical solutions on topics such as just-in time (JIT) compilation, the growing speed-up of hardware, and the advantages offered by the concept of \textit{managed} languages.
Since then, it has become clearer that modern languages demand features which require a considerable runtime support.
These features include the following:

\begin{itemize}
\item \textbf{\textit{Portability}}:
In an ideal scenario, applications should be distributed to customers and deployed in a platform independent format that can be executed on top of specific architectures and platforms.
This helps to reduce the application's time to market.
To provide this feature, the application must be written in such a way that allows its execution on top of an abstract machine which has its own instruction set instead of on top of native platform.
An application can then be executed on a platform if there exists an implementation of this abstract machine that is able to run on such a platform.
Having a different instruction set offers additional advantage.
For instance, final applications' code can be more compact than code written using native instructions because it can represent only the high level concepts that matter to the abstract machine.
To support this feature, a runtime must provide either application's interpretation, ahead of time compilation \cite{Muller:1997:HFE:1268028.1268029,Proebsting:1997:TJA:1268028.1268031,Wang:2011:MAC:2038698.2038704,Oh:2015:BAC:2757012.2757057} or just in time compilation (JIT) \cite{Inoue:2012:AMC:2398857.2384630,Paleczny:2001:JHT:1267847.1267848,Grcevski:2004:JTJ:1267242.1267254}.

\item \textbf{\textit{Dynamic code loading}}:
It is also desirable to support loading new code from different sources (e.g., a network stream, a local file, internally generated code) while the application is running.
Combined with the reflection mechanism this feature is useful in many scenarios such as implementing on-the-fly generation of proxy classes, implementing component frameworks, and supporting the Aspect-Oriented paradigm.
Providing this feature for a MRTE is only possible using an interpreter or a JIT compiler; hence in comparison with the \textit{portability} feature it rules out using an ahead-of-time compiler.
In addition, a new requirement emerges: since code can be dynamically loaded from untrusted source, it is now mandatory to verify the correctness of such code in order to guarantee that it is safe to execute it.  

\item \textbf{\textit{Automatic Memory Management}}: 
Memory management has proven a major source of applications' crashes.
As a consequence, automatic memory management is often a feature of modern programming languages.
It is usually implemented using a \textit{garbage collector}, a general technique that can be implemented following different approaches and often requires some compiler's assistance (e.g., mark-swept, copying, reference-counting).
\textit{Memory allocation} and \textit{garbage collection}, which is the memory reclamation mechanism, are commonly implemented as part of the runtime environment.
The memory manager is in charge, for instance, of allocating the space required by object's instances, and by closures.

\item \textbf{\textit{Improved error handling}}:
Modern programming languages tend to offer support to detect early during the development phase and to avoid at runtime common mistakes such as dereferencing a null pointer or accessing arrays using a wrong index.
This kind of features requires extensive support for handling internal and unexpected errors as well as developer-specified faulty conditions.
Only with some collaboration of the interpreter or JIT compiler is possible to implement such features.
\end{itemize}   

The runtime environment support needed to execute applications written in many modern programming languages is referred as managed because the code used to execute the applications includes not only the business logic but also code to \textit{\textbf{manage}} the memory, the possible errors, and the process of loading, verifying and compiling the code on demand.

A feature present in MRTEs and the methods used to implement it are relevant for this thesis.
In the rest of this section we briefly describe the mentioned feature.

\subsection{Memory Management using Garbage Collection}

Automatic memory management is usually implemented using garbage collection.
In this approach three entities are involved: i) the application which is also known as the mutator in memory management jargon because it is the one modifying the memory content, ii) the allocator which is in charge of reserving space in the heap, and iii) the garbage collector (GC) which reclaims memory which is no longer referenced by the mutator.
Memory allocation follows a ``simple guideline'': a thread belonging to the mutator requests memory, in response the allocator searches for an unused block in the heap, if an unused block cannot be found then the garbage collector is invoked to reclaim blocks of memory, the allocator tries to find an unused block once again.

To understand what the garbage collector does, it is worth looking at how the memory heap is organized.
The heap contains a set of allocated blocks with varying size.
A memory block can represent an \textit{object} as in object-oriented programming or an array, but along this description we use the generic term object as a synonymous of memory block. 
Objects contain primitive values that represent the internal state of applications, but they also contain references to other objects.
Due to the way in which applications allocate memory, after some times the heap has a large set of objects that are connected by references, forming a directed-graph.
In this graph, edges are references and most nodes are just objects.
There is however a different class of node that are known as \textit{roots}.
Roots exist because references are not only stored within object.
Instead, references may be stored in locations such as local and global variables.
If a reference is stored in a local or global variable, it is say that the referenced object $O$ is still useful and the memory it consumes cannot be reclaimed - $O$ is a live object.
As a consequence, any object referenced by a value within $O$ is also live.
In summary, an object $O$ is live or reachable if and only if there exists a directed path from a root node $R$ to $O$.
The mission of a garbage collector is to discover the set of dead objects in the directed-graph and reclaim their memory.

The complexity of writing a garbage collector is reaching a good performance.
A comprehensive description of different garbage collection approaches can be found in ~\cite{Richard2012}.
In the remainder of this section we present a brief overview of the most basic techniques.

\begin{itemize}
\item \textbf{Mark-Sweep collectors}: In this approach, objects are allocated from a list of free blocks.
Once the system decides that some memory must be reclaimed, the GC performs an initial traversal of the object graph starting by the root nodes, following the references and marking all the visited nodes.
Afterwards, the heap is traversed during a second step and every object without the mark is removed from memory.

\item \textbf{Copying collectors}: In this approach the heap is split in two spaces of equal size.
Allocations are performed in one of the two spaces by simply increasing a base pointer.
Once the allocation space is full, the GC is invoked to reclaim some memory.
The GC proceeds by traversing the directed graph from the roots and copying every visited object to the second space.
When the traversal is done, the role of both spaces is exchanged and the objects that were not copied are thus automatically reclaimed.

\item \textbf{Generational collectors}: This approach tries to reduce the part of the graph that must be traversed on every garbage collection cycle.
The approach is based on the observation that most objects have a short life.
As a consequence, it is often enough to traverse only a subgraph of objects that were recently allocated in order to find dead objects to dispose in a faster way.
Following this idea, objects are allocated in a special space and copied to a different heap space once they ``get'' old enough. 
Generational collectors are usually combined with sophisticated variants of mark-swept and copying collectors.
\end{itemize} 

There are many areas to consider in a discussion regarding garbage collection.
For instance, how different approaches deal with concurrent mutators or how is the response time improved by using parallel collectors.
These topics are not mentioned here because such information is not relevant to the purposes of this work.
In fact, the information to take aways is that state-of-the-art GCs tend to split the heap in different spaces each of them implementing a different allocation and garbage collection mechanism.
Also, GCs see the allocated objects as a directed graph where nodes that cannot be reached from the ``roots'' can be safely disposed.

\subsection{How MRTEs limit resource-aware programming?}

In summary, MRTEs tend to offer fewer instructions and less freedom than native platforms.
However, the supported concepts have a higher level of abstraction that usually favor some programming paradigms (e.g., the \textit{invokevirtual} instruction of the JVM is used to allow method invocation as in object-oriented programming).
It is common to simplify tasks such as memory management, concurrent programming, as well as resource and error handling.
This greatly reduces the complexity of developing applications.
On the negative side, applications may suffer some performance penalties and also lack of control.
As mentioned in section \ref{sec:resource-awareness}, the lost of control makes the language inappropriate in scenarios that require further assistance to deal with resources.

Among the core features present in MRTEs that impact the implementation of resource-aware applications we identify: i) automatic memory management, ii) dynamic code loading, iii) support for concurrent programming, and iv) the usage of high-level abstractions that do not map exactly to resources.
In addition, there are implementation-specific limitations that obstruct the development of resource-aware solutions.
For instance, it has been largely discussed \cite{} the lack of modularization in the Java HotSpot implementation of the JVM.
Likewise, there are many dependency relationships among different sections of code within the Java standard library that hinder resource management related tasks \cite{moxi, binder}.

\begin{comment}

Esta seccion sencillamente presenta las principales caracteristicas de un ambiente de ejecucion manejado.
Esta es una seccion de background donde sencillamente se establecen las caracteristicas de estos sistemas con las que tenemos que lidiar y que repercuten en las decisiones que tomamos en nuestro trabajo.

Por ejemplo, debemos discutir como en MRTEs se esconde intencionalmente el aspecto de manejo de recursos para facilitar el desarrollo de aplicaciones.

Discutir en particular la cuestion de la memoria.

Discutir las ideas de carga dinamica de codigo y commo esto se puede usar para cosas buenas como crear component framewroks pero complica las cosas porque ya no se cumplen las condiciones de mundo cerrado.

Esta es una seccion informativo, es util para poner al lector en contexto y para que pueda comprender elementos que vienen a continuacion.

Su longitud estimada es de 1 a 2 paginas.

\end{comment}


\section{Resource awareness in MRTEs} \label{sec:resource-awareness-related-work}
%\section{Basic/System support for resource awareness}

Many solutions to deal with the problems of resource consumption monitoring, control and management have been proposed.
In reviewing the body of knowledge related to this topic, we are interested on solutions with specific properties.
In particular, we only consider those solutions that can be applied to MRTEs.
This section presents a summarized review of different approaches that we can leverage to support resource aware programming in MRTEs.
The objective is to compare these approaches based on a common set of properties in order to determine which solutions better suite the requirements of supporting resource awareness in MRTEs.
In the rest of the section the following properties are briefly discussed for each approach: 

\begin{itemize}
\item\textbf{Type of resource that the approach is able to handle:}
There are different types of computational resources.
Often, monitoring and managing a resource type poses a challenge which is quite different to dealing with another resource type.
In the first place, this happens because the hardware/platform support for resource management varies depending on the type of resource.
However, this is also the result of intrinsic differences on the way resources are consumed.
As a consequence, solutions to face resource related issues are often limited to a few type of resources.
In this thesis, we are interested on the following resources: \textit{CPU}, \textit{Memory}, \textit{Network Bandwidth}, and \textit{IO Throughput}, \todo{disk quotas}.
It is noteworthy that some approaches are able to deal with many resources and others with just one resource.
Adding this to the fact that there is no natural order of importance among the type of resources, we can see that this property is \textit{nominal} and \textit{multivalued}.  

\item \textbf{Portability:}
This property is desired on any software system.
In the case of approaches that support resource awareness in MRTEs, we consider two aspects related to portability.

The first aspect refers to whether a solution can be seamless used on a given execution environment without further modifications.
For instance, some approaches rely on operating system (OS) features.
Other techniques require a modified MRTEs to deliver the desired services.
Finally, there are solutions that do not require features from the OS nor modifications to the MRTE.
In short, we identify three values for this property: \textit{OS Specific}, \textit{MRTE Specific} and \textit{Portable}.
For the purpose of this thesis, we establish a partial order among these values which is based on the superiority of the solution in terms of portability.
It is clear that a solution which is both OS and MRTE independent is the most portable one.
However, it can be argued whether it is better to use an OS specific solution instead of a MRTE specific approach.        
On the one hand, we can see it as a matter of how likely is that a solution will be adopted. 
Hence, it is possible that a solution based on existent OS features would be preferred over a solution based on a MRTE extended with additional features.
On the other hand, MRTE specific approaches can be considered more portable due to the fact that MRTEs are themselves already ported to many OSes.
In this thesis, we rely on this second criteria.

There is a second portability aspect to consider: how easy is to write a contract on resource consumption.
For instance, it is hard to define a contract regarding resource consumption if we want to execute a workload in a target platform while at the same time we control the consumed resources.
The problem arises because there is a potential hardware/software mismatch between the development platform and the target platform.
Hence, writing the contract using architecture dependent metrics is not the proper solution to this problem \cite{no se} (e.g., 10\% of CPU is probably enough to complete the workload in the development platform, but how much is the equivalent value in an other platform?).
On the contrary, using values with the same meaning in both the development and target platforms for writing the contract is a solution that eases the specification of resource consumption contracts.
In this thesis, we have identified few solutions with this feature.
Since all of them are portable regarding the first portability aspect, we call them \textit{fully portable}.
However, it is worth mentioning that the problem of defining contract for specific platforms has been discussed elsewhere using other approaches \cite{otros}.

In summary, portability is an \textit{ordinal} property which can take the following values: \textit{OS Specific}, \textit{MRTE Specific}, \textit{Portable} and  \textit{fully portable}.

\item \textbf{Granularity:}
Traditionally, operating systems have treated processes and threads as units accountable for resource consumption.
More recently, virtual machines and application's containers have served the same purpose.
As a consequence, mature solutions exist for managing resource at process and thread levels.
Unfortunately, these are coarse-grained levels that are not useful in some scenarios.
In MRTEs, it is usually necessary to control and monitor resource consumption using fine-grained approaches.
Four values are used while comparing different approaches in this section: \textit{whole environment}, \textit{managed thread}, \textit{method} and \textit{arbitrary}.
A solution provides resource awareness at the level of the \textit{whole environment} if it is only able to control, monitor or manage how the whole MRTE uses resources.
The granularities \textit{managed thread} and \textit{method} are self-explanatory.
It is only necessary to highlight that if a solution is, for instance, able to handle a single managed thread then it is also capable of handling several managed threads by simply aggregating, probably with an additional performance overhead, the results of many threads.
In this thesis, arbitrary granularity level refers to the possibility of collecting data and managing resource for different concepts.
For instance, monitoring the CPU consumption of several threads plus the consumption of a few methods executed by another thread.
This is also an \textit{ordinal} property where the values are sorted from coarse to fine granularity level.

\item \textbf{Performance Overhead:} 
In dealing with resource awareness, a major concern is the performance overhead required to support the paradigm.
The overhead is usually produced by the need of carefully monitoring and controlling how resources are used.
In general, there exist trade-offs between the performance and other properties such as granularity and portability.
For instance, as finer the granularity as higher the overhead.

Comparing approaches based on this property is nonetheless troublesome due to a number of factors.
In the first place, most approaches have been evaluated using different hardware, operating system, MRTE and benchmark.
Hence the results are not directly comparable.  
Second, some approaches can be applied in the context of MRTEs, but they have been evaluated in other contexts.
Finally, conducting further experiments to evaluate how each approach behaves under similar conditions is not only extremely time consuming but also impractical because some approaches rely too heavily on specific platforms or they are no longer available for experimentation.
Fortunately, most results have been presented as the percent of overhead produced by the addition of resource management capabilities to an already existent system.
Thus, we can use these values as measurements for the comparison.
Along this section, we use the \textit{ordinal} labels \textit{low}, \textit{medium} and \textit{high} to denote the performance overhead.
We also show the numerical value when it is available.
In cases where there is no numerical value published, we discuss the reasons that let us to label a method with a given value.
\end{itemize}

Esta seccion presenta una serie de metodos para gestionar recursos. En primer lugar explica que nos estamos limitando a accounting y reservation. Despues comienza discutiendo metodos general que no son solo para MRTE. A continuacion se hace un recorrido por metodos que estan orientados a MRTEs pero no a abstracciones en especifico. Esta ultima parte va a estar dificil de hacer peor veremos. 

La siguiente es una lista no exahustiva de cosas a discutir/mencionar

\begin{itemize}
\item CGroup \cite{Soltesz:2007:COS:1272998.1273025}
\item FreeBSD \cite{Kamp00jails:confining}
\item Resource containers \cite{Banga:1999:RCN:296806.296810} 
\item MVM \cite{czajkowski_multitasking_2001}
\item KaffeVM \cite{back_processes_2000,Back:2005:KJR:1075382.1075383}
\item JRes \cite{czajkowski_jres:_1998}
\item J-Raf2 \cite{Binder200657,Hulaas:2008:PTL}
\item garbage collector memory accounting \cite{Price:2003:GCM:829515.830545, gael}
\item A Framework for Reducing the Cost of Instrumented Code \cite{citeulike:481405} 6\%
\item Binder \cite{Binder:2009:PPV:1464245.1464249,Binder200645}
\item Overseer (Hardware performance counters) \cite{DBLP:conf/pppj/PeternierBBP11}
\item sampling, osgi, kouther, \cite{Maurel:2012:AME:2304736.2304763}
\item dtrace
\item \dots
\end{itemize}

Aqui entonces vienen los resultados de la discusion de esta parte del estado del arte.
Estas conclusiones deben mostrarse preferentemente en la forma de tabla donde se contrasten las diferencia en diferentes variables. Algunas de las variables para comparar son:

\begin{itemize}
\item es usable en MRTEs?
\item Puede manejar time shared resources y otros?
\item cual es el rendimiento?
\end{itemize}

Al final la seccion concluira mostrando como ninguna de las tecnicas es completamente satisfactoria. Claro, para medir satisfactoriedad debemos constrastar sus caracteristicas contrar caracteristicas deseadas que probablemente se definiran en la seccion de resource aware o en el principio de esta seccion. Esta parte tiene que hacerse de forma tal que lleve a la conclusion logica de que algo mas es necesario. Pero tiene queser creible.

\todo{This makes no sense at all}

%\input{chapter1/fig/images.tex}

\section{Resources Reservation}



\begin{table}
\caption{Esto es el caption}
\begin{tabular}{|l|l|l|l|l|}
\hline % header
 & \textbf{Resources} & \textbf{Portability} & \textbf{Granularity} & \textbf{Overhead} \\ 
\hline % Resource Containers
\multirow{4}{*}{\textit{Resource Containers}} & CPU & \multirow{4}{*}{OS Specific*} & \multirow{4}{*}{Thread} & \multirow{4}{*}{low} \\
& Memory & & & \\
& Network Bandwidth & & & \\
& IO Throughput & & & \\
\hline % cgroups
\multirow{4}{*}{\textit{CGroups}} & CPU & \multirow{4}{*}{OS Specific} & \multirow{4}{*}{Thread} & \multirow{4}{*}{medium} \\
& Memory & & & \\
& Network Bandwidth & & & \\
& IO Throughput & & & \\
\hline % jails + resource limits in FreeBSD
\multirow{4}{*}{\textit{Jails, FreeBSD}} & CPU & \multirow{4}{*}{OS Specific} & \multirow{4}{*}{Process} & \multirow{4}{*}{medium} \\
& Memory & & & \\
& Network Bandwidth & & & \\
& IO Throughput & & & \\
\hline % MVM
\textit{MVM*} & Memory & MRTE Specific & Thread* & low (0.5 \%) \\
\hline % KaffeVM
\multirow{2}{*}{\textit{KaffeOS}} & CPU & \multirow{2}{*}{MRTE Specific} & \multirow{2}{*}{Thread*} & \multirow{2}{*}{medium} \\
& Memory & & & \\
\hline % JRes
\multirow{4}{*}{\textit{JRes*}} & CPU & \multirow{4}{*}{OS Specific} & \multirow{4}{*}{Thread} & \multirow{4}{*}{medium* (18\%)} \\
& Memory & & & \\
& Network Bandwidth & & & \\
& IO Throughput & & & \\
\hline
\multirow{2}{*}{\textit{Garbage Collector*}} & \multirow{2}{*}{Memory} & \multirow{2}{*}{MRTE Specific} & Thread & \multirow{2}{*}{low(1\%)} \\
& & & Classloader & \\
\hline
\textit{J-RAF2*} & CPU & Portable & Thread* & medium (37\%) \\
\hline
\multirow{2}{*}{\textit{Binder} \cite{Binder:2009:PPV:1464245.1464249,Binder200645}} & CPU & \multirow{2}{*}{Portable} & \multirow{2}{*}{Arbitrary*} & \multirow{2}{*}{high (80-300 \%)} \\
& Memory & & & \\
\hline
\end{tabular}
\end{table}

%Computer systems consume resources to execute an application.
%The nature of resources varies in the sense that at certain point in time a given resource can be shared among applications or not.
%The classical example of shareable resource is memory.
%On the other hand, we have CPU which cannot be shared.

%Resource limitation has been a concern in computer science for a long time because writing
%applications for an environment with unlimited resources is easier than for real computer systems.
%Indeed, lots of work have been done to hide the resource-limitation problem, some examples are: multiplexing CPU usage and creating virtual memory subsystems.
%Operating systems and others runtime environments perform quite well offering resource abstractions
%to developers.
%However, resource limits are still there and they can produce applications failures.
%Moreover, the performance of applications is affected because resources management interfaces use
%to be quite general~\cite{Engler:1995:EOS:224056.224076}.

%Every single application is subject to failure due resource limitations. This risk is increased in embedded devices and other kind of resources constrained computer's systems. For many applications a failure related to resources limitation is not a big concern.
%Indeed, users can be unhappy if their text editor turn slow or even fail but most of the time nothing really important depends on it.
%Nevertheless, there are other applications where such failures are unacceptable.

A system is considered critical if, for instance, its failure can produce financial losses, environmental damage, injuries of human beings or others  \cite{Knight:2002:SCS:581339.581406}.
A more general definition is centred in businesses.
For a business, a software application is critical if its failure stop the proper running of business \cite{Knight:2002:SCS:581339.581406}.
For example, a system which handles customer's orders is critical for an online-selling business like eBay or Amazon.
Another domain, where failure is not acceptable, is real-time applications.
Real-time applications are those which demand an instantaneous response to external events \footnote{There are two kinds of real-time applications, 1) soft real-time applications and 2) hard real-time applications. Response time is relaxed in the former.}.
Critical applications and real-time applications are two different sets \footnote{A computer game is real-time application because the need of real-time rendering, but it is not a critical application. On the other hand, a data-center is critical for most companies, but it is not a real-time application}.
However, there are also applications that falls in both categories, but it is important to remember the difference because it has several implications.

Even for non-critical and non-real-time applications, resource limitation is still important because it can prevent normal execution.
%Of course, this is a matter of logic but in today's dynamic application it is a major concern.
In the past, developers could easily define hardware requirements.
However, nowadays dynamic applications can be deployed in resources constrained computer systems and several developers can add components at runtime; thus, it is not easy for developers to state when a given component will work.
Resource limitation should not be a concern just for component's developers but also for middleware's developers.

Response time is the major concern in real-time applications; therefore, CPU tends to be the main resource to ensure.
For critical applications  as well as for general applications, resource's needs vary.
Sometimes, it is more important to ensure network bandwidth, memory, or disk quota and so on.
It depends on application requirements.
The common factor is that the resource must be available when needed.
Resource reservation is the only way to ensure availability.
It can be as simple as marking a region of memory as owned by a single task or as complex as providing a task's scheduler for real-time systems where real-time applications have higher priority.
There is a lot of related work in real-time's domain \cite{Kirsch:2005:PMR:1064979.1064986, Zuberi:1999:ESR:319151.319170, Higuera-Toledano:2012:YRJ:2388936.2388943, Alonso:2006:FJR:1167999.1168022}.
Another interesting approach to resource management is the exokernel concept \cite{Engler:1995:EOS:224056.224076} where the notion of resource management and resource protection is decoupled.

In Java world, resource reservation is almost non-existent.
Resources are considered a low-level concept which must be handled by the runtime environment.
Although most of the time this is a proper approach, there is an increasing interest in dealing with resources within Java applications.
In the following sections we present previous approaches to the problem of resource reservation in Java domain, but also in operating systems and cloud computing.

\subsection{Resource Containers}
Banga et al. propose in \cite{Banga:1999:RCN:296806.296810} a new operating system abstraction for resource management.
This abstraction, called resource containers, allows a fine-grained control over definition of independent task and its resources.
The intended target of their work is server systems; but it is still relevant to other applications.

Banga et al. state that in current operating systems there is an incorrect association between protection domain and resource principals.
Indeed, the protection domain of a task is equal to the resource principal of this task.
In current operating systems, a process is the abstraction for both, protection domain and resource principal.
Reasons to use different entities for protection domain and resources principals can be, among others, security and performance.
This kind of design decisions is very common in HTTP servers and time-consuming applications.

Many examples are presented in \cite{Banga:1999:RCN:296806.296810} to highlight the problems.
We show a summary of some examples in the following paragraphs.

In applications accessing the network, the operating system kernel plays a major role.
In such a case, the process is the correct unit for protection isolation.
However, resources used by the kernel to satisfy application's requests are not charged to application.
This issue has been addressed by both monolithic and micro-kernel operating systems \cite{Bao:2008:HPI:1384529.1375484}.
The conclusion is that in client/server architecture resources consumed by the server in behalf of a client must be charged to resource principal of the client.
This avoids denial-of-service attacks and is useful to ensure QoS.

Some applications are split into different processes to guarantee fault isolation between component.
These applications still perform a single task; hence, the natural protection domain is different to the desired resource principal.
In such a case the desired unit of resource management is bigger than a process.
However, nowadays operating systems manage resources per process instead of per task; therefore, the previous behaviour is not easy to implement.
A similar scenario is common in extensible frameworks written in unsafe language where third-party extensions can be loaded.
In Java world, this is a common scenario too because even if Java is a type-safe language, there are other ways to produce failure due to lack of isolation.

Another scenario is possible.
A single process can perform several independent tasks to avoid context-switching and to reduce IPC overhead.
In this case the resource management unit is smaller than a process.
Indeed, the resource unit is the set of all resources used for the process to accomplish a single task.
This setting is common in current Java-based middlewares because many components share the memory heap inside a single JVM.
The common solution is to provide an additional isolation unit on top of Java's classloaders.

Even in more recent operating systems where thread assume some roles of resource principals (e.g CPU usage is not longer assigned to processes but to threads) process is still the resource principal for memory and files.
However, the problem is not the size of resource principal.
For instance, suppose we make threads the resource principal and processes the unit of protection domain, in such a case we still have the following scenario: we can multiplex a single thread between several independent tasks and hence one task is not isolated from the rest.

The argument is that tying resource principals to static concepts as process, thread or Java instance is wrong because it reduces the set of solutions.
A flexible definition of resource principal should provide developers with a better way to deal with different concerns like security and performance.

A resource container is the new abstraction proposed in \cite{Banga:1999:RCN:296806.296810} to deal with resource principal for tasks. It is an entity which contains all the system resources used by a particular independent task \cite{Banga:1999:RCN:296806.296810}. Resource containers have attributes like scheduling parameters, resource limits and so on.

As was mentioned, in classical operating systems there is a fixed binding between processes/threads and resource principal.
In short, resource consumption of a process/thread is charged to the associate process.
However, resource containers allow having dynamic binding between threads and resource principal and this under the control of application.
In this way several scenarios are possible, for instance:
\begin{itemize}
\item Threads of different processes share a resource container.
\item It is possible to multiplex a thread between several resource containers.
\end{itemize}
The resource usage is charged to the correct container and allocation algorithms can manage consumption in principals using different policies.
Containers are just a mechanism to provide resource management to application developer and hence it can be used with several policies.
Resource containers also form a hierarchy.
Resource usage of a child is constrained by attributes of parent container.
This allows easy controlling complete subsystems by defining special policies.

To support containers, new operations are defined \cite{Banga:1999:RCN:296806.296810}. We briefly present the operations because a detailed explanation is not relevant to this work.
\begin{itemize}
\item Creating a new container.
\item Set a container's parent.
\item Container release.
\item Sharing containers between processes.
\item Access container attributes.
\item Access container usage information.
\item Binding a thread to a container.
\item Binding a socket or a file to a container.
\end{itemize}

Note that in most middlewares implemented in Java we have a scenario were resource principal is bigger than the unit of isolation.
The resource principal is the whole Java instance\footnote{This is not the case if you use something like Multitasking Virtual Machine} and the unit of isolation is the classloader. Independent activities share the resource principal.

\subsection{Real-time programming in Java}
Real-time is a term used to describe the desired behaviour of some applications.
In such applications, there are real-world time requirements.
For instances, in a computer game the desired frame rate can be established in 30 frames per second, therefore the renderer must render a frame in 2 seconds to avoid animation freezing.
Time failure occurs if this requirement is not met.
The previous example is considered a soft real-time application because a time failure is not considered catastrophic for the execution of such an application.
On the other hand, hard real-time applications have strict time requirements.

Designers of real-time applications need deterministic time response to articulate a system with desired properties.
Designers of execution environments, like real-time operating systems and so on, devote huge effort to reduce non-deterministic performance effects because by lowering non-determinism the number of runnable RT applications increase.

The nature of Java language and the JVM specification introduce non-determinism in time-response.
Three sources of non-determinism are 1) dynamic class loading, 2) just in time compilation and 3) garbage collection as implementation of automatic memory management.
Dynamic class loading affects because a class representation is loaded first time the application uses that class.
Indeed, delay in application's execution and time failure can take place if the application load a class while it is responding to a real-time event.
Time failure can be important even for soft RT application because loading a class may trigger additional dynamic class loading in a recursive way.
This unlikely effect is worst because most JVM implementations use just in time compilation to speed up execution.
Common techniques like adaptive compilation, where hot-spot bytecode is recompiled, may slow down RT application's execution with undesirable effects on time response.
Finally, the garbage collector plays a major role in non-determinism of Java application.
Most garbage-collection techniques perform a stop the world step at some point.
Time spent collecting depends on many factors like rate of living objects, size of the heap and so on. Anyway, it is impossible to predict when a collection will be trigged and how long it will take.

The Real-time Specification for Java was created to address some of the limitations of Java that prevent its use as RT execution environments. The RTSJ addresses several problematic areas, including scheduling, memory management, threading, synchronization, time, clocks, and asynchronous event handling.

RTSJ is related to resource reservation because one of its main goal is to guarantee deterministic CPU time devoted to RT applications. Other features of RTSJ like threading system, synchronization, timer and event handling are not directly related to resource reservation; hence, we will skip it in the section.

Memory management is the major source of non-deterministic response time in Java applications. The overhead imposes by garbage collection is high; therefore, only applications with larger scale and loose timing requirements can afford to rely on traditional GC technology.
RTSJ addresses this problem with two different approaches.
The former is providing two new memory regions to applications and the latter is providing a garbage collector with deterministic behaviour.
It is important to note that the concern of these approaches is CPU reservation and have nothing to do with memory reservation.

RTSJ provides two new memory regions in addition to the standard Java heap.
The new memory areas are an immortal space and scoped spaces.
An immortal space is shared between threads and no collection is done over objects in this space.
Allocated objects in immortal space live until application's termination and thus an immortal space is a limited resource.
On the other hand, scope spaces are created with fixed size and destroyed by application at programmer will.
All objects in a given scoped area are released at the same time.
RTSJ define rules to control the way objects in different areas interact with each others.
Additional rules define the way objects in scoped space are finalized and when a memory area can be reused as scoped area.
All these rules limit the usability of the approach because it enforces changes in Java programming model.
Using immortal and scope memory areas is only recommended when GC pauses are non-acceptable.

The second approach is useful when short GC pauses are acceptable. Core idea is to use a collector with the following properties:
\begin{description}
\item \textbf{Property 1}. No single GC pause exceeds some maximum upper bound.
\item \textbf{Property 2}. GC will consume no more than some percentage of any given time window by controlling the number of pauses during that window.
\end{description} 
This means that RTSJ ensures a minimum percentage of CPU time to the mutator over any time interval.
For instance, if the JVM implementing RTSJ was configured to ensure 80 $\%$ then in 60 seconds at least 48 seconds will be devoted to the mutator.

Metronome GC is a solution developed as part of WebSphere Real Time \cite{Bacon03themetronome:}.
Collection is scheduled at allocation time in most garbage collection methods, but this result in non-deterministic long GC pauses.
Instead, the Metronome GC uses a time-based method of scheduling, which interleaves the collector and the application on a fixed schedule.
The approach consists on dividing time in quanta of 500 microseconds.
A quanta is devoted to a single activity, the mutator or the GC.
The execution environment enforces a minimum amount of quanta to mutator.
This ensure that Metronome GC conforms to property 2.
Metronome performs collection during many short GC pauses, in order to do that the work is divided into several parts which require using write barrier and others technique to avoid costly operation like copying \cite{Bacon03themetronome:}.

It is important to note that developers can specify the desired percentage for application and GC execution.
However, using a number too high can produce out of memory exception although the rate of living object remains low. The reason is that the GC needs time to do its job.

The main advantage of using this approach instead of immortal and scoped areas is related to the programming model.
Any program written in Java can be executed with Metronome GC.
Developers need to learn nothing.
It is really an important advantage considering the way the classes library is implemented.
Indeed, the core Java library is implemented with the assumption of GC existence and hence a lot of objects with short live are created and without a GC those libraries are useless.

RTSJ uses other techniques not related to GC to guarantee deterministic response time. These techniques deal with the problem of just in time compilation. Although it result an interesting topic, it is not related to the current work. 


\subsection{KaffeOS and MVM}
The most common concern related to resource reservation in Java is memory reservation.
The reason for this is the overhead imposed by automatic memory management.
A natural approach to face such a problem is partitioning memory into different areas and assigning an area to every single task.
In this section, we present two works which follow this approach.

\emph{KaffeOS:} Despite of its name, KaffeOS is a Java Virtual Machine which provide the concept of process at virtual machine level \cite{back_processes_2000}.
It offers abstractions like data isolation, safe termination, process forking and inter-process communication.
KaffeOS isolates the data of each process by providing a per-application user heap where reference to a different user heap is forbidden.
Figure \ref{fig:kaffeOSMemoryLayout} shows an example of configuration with several heaps.
In KaffeOS, there are three classes of heaps, 1) user heaps where process's allocations take place, 2) shared heaps with fixed size used by many processes and 3) a global kernel heap used to allocate special data structures.
Access rules are enforced through write barriers to ensure protection, safe termination and per-application garbage collection \cite{back_processes_2000}.

%\begin{figure}
%\caption{Memory layout in KaffeOS with three processes and some allocated objects}\label{fig:kaffeOSMemoryLayout}
%\centering
%\kaffeOSMemoryLayout
%\end{figure}

The main goal of KaffeOS is to offer isolation between tasks inside the JVM.
However, it enforces a wrong programming model to deal with components.
Using the process abstraction inside Java is just skipping chances that any MRTE may offer.

\emph{MVM:} Czajkowski et al. present the multitasking virtual machine in \cite{czajkowski_multitasking_2001}.
The goal is to execute isolated Java virtual machine instances within a single operating system process.
This is aimed to decreasing the memory consumption by sharing data structures like cache of code, constant pools and so on.
Furthermore, starting new applications is speeded-up because the initialization process is not needed.
As the goal includes isolation among instances, direct object sharing is forbidden.
The approach has advantages, but also drawbacks.
On one hand, it is remarkable that the Java programming model is not modified.
On the other hand, the usability in middleware contexts is limited because if you want to isolate components in different instances then communications is only possible through Remote Method Invocation with all the negative impacts it implies.

In MVM, the garbage collector has been modified to guarantee separation of resources between tasks.
The collector uses a generational approach where the heap for old generation is shared among tasks.
The new generation is represented with three spaces: 1) Eden space , 2) from-space and 3) to-space.
The three of them with equal size.
New objects are allocated from the Eden space and the from-space contains objects which survive some collections but are still young.
The role of to-space is the traditional in semi-space collector. Like in any generational collector when an object become old it goes to old space.
Figure \ref{fig:mvmMemory} shows the scheme.

The disadvantage of using MVM to build middlewares is the high cost of RMI because by isolating Java instances, the communications between components running in different instances is slowed-down.

%\begin{figure}
%\caption{MVM with three tasks}\label{fig:mvmMemory}
%\centering
%\mvmMemoryLayout
%\end{figure}


\subsection{JAMUS} \label{JAMUS}
%RAJE is an extension to the Java 2 platform implemented in the context of RASC project.
%The project's aim is to support high-level resource-aware environments and applications.
%To accomplish its goals, RAJE provides mechanisms to observe resource consumption and to use the information at any stage of components life-cycle \cite{guidec:hal-00342142}.

The Java Accommodation of Mobile Untrusted Software (JAMUS) is a research platform dedicated to support the deployment of "untrusted" software components such as application programs and applets.
Indeed, the source of software components could be "untrusted"; hence, it is mandatory to provide a safe runtime environment for components.
JAMUS provides such an environment and it also guarantees QoS properties which are related to resources consumption. 

%As a consequence, emphasis is put in JAMUS on providing a safe and guaranteed runtime environment for components, as well as guaranteed QoS as far as resource availability is concerned. 
%By providing monitoring facilities RAJE become a useful platform to support adaptive systems, security-oriented systems and QoS-oriented systems.

JAMUS follows a contractual approach to resource control.
At deploy time, a software component must specify explicitly what resources it will need at runtime.
On one hand, signing the contract means that the candidate component requests a specific service from the JAMUS platform.
This contract also states that the component will not use resource others than those explicitly mentioned.
On the other hand, the platform promises to provide all resources the component requires, but it reserves the right of punishing offensive components~\cite{JAMUS2002}.
%Based on these contracts, JAMUS provides quality of service regarding resource availability.
%It also provides relatively safe runtime environment, since no component can access or monopolize resources \cite{JAMUS2002}.
Finally, components can specify their requirements regarding resource utilization in both qualitative and quantitative terms.
As an example of qualitative requirement we have access right to sockets.
Resource quotas are good examples of quantitative requirements.

JAMUS implements a resource broker.
Its role is to guarantee the availability of resources for deployed components.
At start-up, it receives a description of available resources.
%JAMUS provides a series of interfaces and classes that enable the specification of resource access conditions as "resource utilization profiles" \cite{JAMUS2002}.
With that information, the broker builds and maintains a structure which represents its perception of resource availability.
A component that applies for being deployed on the JAMUS platform must first pass the control-admission process.
The requirements of this component must be expressed as a set of resource utilization profiles.
Those requirements are examined by the broker to decide if the component can be admitted on the platform.
Admission is based on a simple rule: a component can be deployed if the resources it requires are available on the platform in sufficient quality and quantity.
Resources that a component requires are reserved for its sole usage until it reaches completion.
The resource reservation is performed by updating the broker's perception about resources availability \cite{JAMUS2002}.

After a successfully admission, a component start running on the platform.
However, once a component has been accepted, it is considered as non-trustworthy.
Indeed, an offensive component may attempt to access resources it did not explicitly ask for.
To prevent such attacks, every component runs under the control of a dedicated component monitor, whose implementation relies on RAJE which is described in section \ref{RAJE}.
When a monitor observes that the component is acting against the contract, it can punish the component. Sanctioning a component is done by forbidden the resource, or by killing the component.

\subsection{Dynamic resource reservation} \label{CloudReservation}

Sim\~{a}o et al. present in \cite{Simao:2012:VEJ:2310096.2310158} the Adaptive and Resource-Aware Java Virtual Machine, ARA-JVM, a work in progress to deal with resource management for cloud computing using high-level virtual machines.
The goal is to build a cloud computing platform with special concern in resource consumption.
The solution involves modifying a JVM to deal with nodes, task migration between nodes and transparent communication among nodes.
From the perspective of resource-aware platform, the approach is interesting because resource reservation for application is done dynamically.
An additional advantage of this approach is that it is transparent to application's developer.

ARA-JVM is built upon several runtime instances.
Each instance cooperates by sharing resources.
To implement resource sharing, a global mechanism is needed to make both simple and complex adaptations.
Moreover, a MRTE with enhanced services is required at every node and upon that, a mechanism to aggregate individual VMs is used~\cite{Simao:2012:VEJ:2310096.2310158}.

Applications are monitored to obtain precise information about resource consumption and application's progress.
Such information is used to build a per application profile.
When an application needs more resources, the platform selects an application with \textit{low} consumption which \textit{donates} part of its resources \cite{Simao:2012:VEJ:2310096.2310158}.
It is important to note that moving resource can be explicit or implicit.
For instances, by reducing the heap of certain application the memory released can be used by another application.
In such a case the resource was moved in an implicit way.
On the other hand, resources are explicitly moved when a new tenant is created.

The current implementation is based on JikesRVM.
On it, resource monitoring has been implemented in the form of JSR 284.
Likewise, they use an external language to provide policies to the virtual machine at start-up time.
The authors present experiments related to heap size modification where several policies about the growing ratio of the heap are compared~\cite{Simao:2012:VEJ:2310096.2310158}.

Although, the dynamic handling of resources by discovering patterns of usage is an approach highly valuable and the research is still a work in progress, there are two problems with the approach.
The former is a poor definition of application/component.
The latter is lack of good experimenting scenarios.


\subsection{Discussion}

The most common solution to resource reservation per application in Java is based in isolates. This approach is implemented in MVM and KaffeOS. Although it is useful for some middleware like web server where the communications between applications is low, the approach is impractical for component-based applications due to the high performance overhead.

There are some promising solutions for resource reservations based on implicit reservations, like the approaches presented in sections \ref{JAMUS} and \ref{CloudReservation}. Both solutions rely on resource accounting at virtual machine level. It is not clear how to extend the second solutions to deal with resource others than memory and CPU time. In the former solution the reservation of some resources like network bandwidth need a combination of resource monitoring and resource management policies. The performance overhead can be high. Moreover, it can be hard for application developers to specify in the contract the amount of resource a component needs.

Resource containers is a promising approach to resource reservation within middleware but this abstraction cannot be implemented in the same way as was proposed for operating systems. The problem is that the approach requires using a complex API. This fact compromises the Java programming model and this situation is unacceptable because 1) previous developed components cannot take advantage and 2) abstractions of resources is compromised. The trend in programming languages is using dynamic languages or static languages with automatic memory management so, it is a nonsense modifying the Java programming model to reintroduce resource management. Of course, critical applications need a mechanism to avoid failing because resource limitations. However, there are other options like using DSLs to specify resource reservation concern and some weaving mechanism. The mechanism of resource containers for Java does not need being as flexible as the API for operating systems. The main concern is decoupling the resource principal of any fixed structure like thread or classloader.

\section{Resource accounting in Java}
In the context of today middleware systems, resources limitation is an important concern because it is easy to develop offensive applications which affect QoS. Resource-aware programming is about providing application developers with a way to change its behavior at runtime to take care of resource limit violations. The key to support resource-aware programming is resources accounting. Knowledge about the amount of resources consumed by an application can be used by the application to control itself or by an external agent to control offending applications.

Traditionally, resources accounting has been performed at operating system level because this is the normal environment to run applications. Resources accounting is easy to implement inside the operating system. The reason to this is the classical view of operating system as a software layer to abstract, protect and multiplex resources. Moving application's execution to a higher layer pop up a problem related to resources accounting because MRTEs do not have direct access to real resources. Additionally, MRTEs offer higher abstraction like type-safe systems. Although type-safe systems do not remove the necessity to application's isolation, it offers chances to lightweight components communications. For instances, it offers the chance to avoid using complex interprocess communication mechanism. However, as some objects can be shared among applications it become hardest to account for some resources.

Java virtual machine specification was built upon the idea that an operating system process is the resource container for a single java application so, you need as many JVM instances as java applications. However, JVM platform has been widely accepted like a tool to middlewares building and the notion of Java instance per Java application was rejected. Main reasons to this are related to language features. The problem with the new approach is the introduction of many QoS risks and even security risks. Isolation is a major concern in Java middlewares but addressing isolation at resources usage level is still a challenge.

Resources accounting at virtual machine level is highly dependent on virtual machine implementation. For instances, thread's model affects the way CPU usage is accounting. In a virtual machine like Kaffe you can apply sampling method \cite{Nagpurkar:2006:ERP:1132462.1132465} but it is a better and even cheaper solution to perform direct accounting. The same problem emerge in memory accounting, memory-management policy you use will affect the accounting subsystem. Even more, the target operating system affect the resources accounting subsystem. For instance, in Unix family the \textbf{/proc} pseudo-filesystem provide information about resource usage but the exact location and the amount of information vary among implementations. This particular issue is really important for applications' developers because not all accounting systems offer the same accuracy.

There is not ubiquitous Java virtual machine implementation. Instead, there are many implementations with different aims and providers. However, there is not official specification for resources accounting in Java. This mean developers can rely on specific solutions because lack of portability. Even more, different implementations at virtual machine level can provide disparate accuracy and services. For developers this mean rewriting resources accounting concern for different Java API.

It is not an easy task ensure the quality of accounting. There are three agents running inside any MRTE, 1) the mutator, 2) the garbage collector and 3) the supervisor which spend time jitting code and so on. The way this can interfere in accounting depends on implementation details. Some questions need to be answer, for instances:
\begin{itemize}
\item Is the garbage collector using a stop the world technique?
\item Is the virtual machine just in time compiling the code or interpreting?
\item How is implemented the threads' model?
\end{itemize}
However, the way developers use the virtual machine is even more important because considering that a single JVM instance can handle several user applications has a huge impact on resources accounting. For instance, if an object is shared for two components (a client which created the object and a server which need a reference) it is hard to define which component must carry on with the accounting for such object. It is even hardest to charge a component with the time spend by the garbage collector to release this object. Additionally, we must charge some application with spent time during garbage collection but the question is to know which application. We can generalize by noting that there are two main reason which make accounting a hard problem, the former is that implementation details can distort the accounting, the latter is that there is not formal definition of resource container or task in Java middleware. Developers started using JVM as platform for running middlewares but JVM specification did not evolve to support resources accounting in the new context.
section
In this chapter we present some methods to perform resources accounting in Java. There are two main approaches to resources accounting in Java 1) at user level and 2) at virtual machine level. Both approaches will be discussed in the following sections. 

\subsection{Portable solution by bytecode rewriting}
A well-known technique to resource accounting in Java is rewriting the bytecode of applications. This process can be done in three different moments.
\begin{enumerate}
\item At compilation time.
\item In a post-compilation stage.
\item At runtime. 
\end{enumerate}
By rewriting the bytecode at runtime, while the application is loading classes, an additional benefit is obtained because third party code could be used. In this approach at least one new Classloader must be defined in order to instrument the code. The Classloader injects code to estimate resources usage. The function of the injected code is to increment counters of resource usage. As it is impossible to know the exact execution path of a given method the solution must rely on control-flow graph. The classloader injects code at begin (or end) of every basic block in order to increment the counters. For some methods it is possible to infer statically the number of instructions to execute or the amount of memory to use but in general this kind of statical analysis is not available due to the complexity of algorithms. The range of operations that can be inserted vary from a simple counter update to calls to complex APIs. However, the concept is the same.

A function to calculate n-th Fibonacci number is shown in listing \ref{lst:fib}. The associated control-flow graph and basic blocks is shown in figure \ref{fig:fibFlow}. For every basic block the instructions to be injected are shown. A similar process is used to account the memory consumption.

Advantage of this approach is portability because the entire solution can be implemented in Java as part of a middleware. In place of the simple statement we are using a more complex code can be injected in order to implement an observer pattern. The observer can execute actions to stop the abnormal resource's consumption \cite{czajkowski_jres:_1998}.

Disadvantage of bytecode rewriting is related to the high performance penalty imposed to applications. The performance penalty emerge in two dimensions, the former is CPU consumption because of the new instructions to execute and the latter is memory usage. Czajkowski et al. showed in \cite{czajkowski_jres:_1998} that the overhead can be higher than 15\% if the amount of allocated objects is high but this is only for memory usage accounting. Binder at al. in \cite{binder_portable_2001} found an overhead around 25\% for CPU usage accounting. According to Hulaas et al. in \cite{Hulaas:2004:PTP:1014007.1014024} an overhead of 40\% emerged when an approach based on bytecode rewriting is applied to SPEC JVM98.

%\begin{lstlisting}[caption={Finding the n-th Fibonacci number}, label={lst:fib}, language={Java}]
%int Fibonacci(n)
%	if (n < 2) return n
%	fi = 0
%	fj = 1
%	index = 2
%	while (index <= n) 
%		tmp = fi + fj
%		fi = fj
%		fj = tmp
%		index ++
%	return tmp
%\end{lstlisting}

A technique to profile Java Application is presented in \cite{Dmitriev:2004:PJA:974043.974067}. The basic approach is using bytecode rewriting to monitor resources usage. This work states that bytecode instrumentation impose high overhead over running systems so, a better way to handle monitoring is needed. In this approach, instrumentation's code can be inserted or removed at runtime. The approach is named dynamic bytecode instrumentation. Data collected in this way are presented in calling context tree format \cite{Ammons:1997:EHP:258916.258924}. The mechanism cannot be implemented without virtual machine support because semantic of Java virtual machine does not allow it. The author modified Hotspot Java VM to perform bytecode rewriting under demand, acting like a server. Most work inside JVM involved tunning the well-known mechanism of swapping between jitted code and bytecode interpretation.

The solution is composed by two components, the former is a JVM which acts like a server and the latter is a client which instrument the code. A brief explanation is given below:
\begin{enumerate}
\item Client sends information about what is the root method to instrument.
\item Server obtains call subgraph and send this information to client.
\item Client rewrites bytecode in these methods and send back bytecode to server.
\item Server swaps the current running bytecode by the instrumented one.
\end{enumerate}
Advantages of this approach are 1) lower overhead due dynamic bytecode rewriting and 2) dynamic discovering of call subgraph presenting results as resources consumed by a particular functional task.

Although the original intend is profiling Java application one can think in using the mechanism to monitor resources usage. A lightweight monitoring technique can be used and under certain conditions dynamic bytecode instrumentation can be triggered for a suspicious task to obtain a fine-grained measure. It is easy to rollback the modification at any point to obtain performance advantages.

Bytecode rewriting is useful to account resources in any entity we consider an independent task. We are not restricted by classloaders. We can even consider a thread is multiplexed between many independent task. By knowing the API used to multiplex the thread we can generate the proper instrumentation code.

\begin{figure}

\caption{Control-Flow Graph with Basic Blocks}\label{fig:fibFlow}
\centering
%\fibControlFlowGraph
\end{figure}

\subsection{Solutions at virtual machine level}
A different approach is making changes at virtual machine level to account for resources usage. The exact nature of this method depend on virtual machine specification and implementation. For instance, some virtual machines have a specific way to deal with threads so, resources accounting is more or less hardest to implement. In some published results the programming model is not preserve and this fact changes the way resources are handled. As there are many considerations, it is hard to generalize. Instead of that we prefer to present related works which highlight trends in this field.

\subsubsection*{Resource accounting in KaffeOS}
In KaffeOS two resources are monitored, the former is memory consumption and the latter is CPU usage. Resources accounting is implemented inside the Java virtual machine as part of the processes handling policy \cite{back_processes_2000}.

Memory accounting is easy to implement because every process has its own user heap so, the associates allocator and collector can account memory in every allocation and collection cycle. In fact, the amount of memory used is the sum of objects' size in user heap. The memory used in shared heaps is charged to all processes pointing to the heap. Finally, the kernel has been carefully implemented to keep the number of kernel objects low and many of them are allocates inside the user heap, for instance, the process data structure is allocate inside process's heap \cite{back_processes_2000}.

As section \ref{Kaffe} explains, Kaffe threading model is implemented in user space so, CPU accounting is straightforward. Accounting is done per-process and both the user time and kernel time is measured. This is easy to implement because the separation between user heaps and kernel heaps and their collectors allows accounting CPU in kernel mode. The accuracy of CPU accounting is increased by minimizing the time spend in no-preemptible sections \cite{back_processes_2000}.

\subsubsection*{Modifying the garbage collector}
A different approach to memory accounting is modifying the garbage collector. Price et al. \cite{Price:2003:GCM:829515.830545} presents a solution where objects are no charged to its allocator but to any task having a reference. In their approach a single heap is used for all task and objects sharing semantic is respected. The main modification is in the tracing algorithm. For every task a set of roots is created. This set contains static fields of classes and stack maps of thread. A tracing with this set of roots is performed and memory consumption for the task is the sum of reachable objects. After repeating this procedure for every task, a final tracing is executed to find uncountable references. Due the nature of the modification, the task's order matter so, a shared object could be charged to different tasks. To avoid this problem the authors introduce uncountable references which stop the tracing algorithm and are charged to the allocator task.

As the tracing stage varies between collector this methodology must be adapted. A particular challenge which need further modification are generational collectors. The technique is no applicable to conservative collectors and reference counting collectors.

Finally, the overhead imposed for this technique is around 2 $\%$ for experiments executed. However, we consider that more realistic benchmark can be applied. The main advantage of the approach is that Java programming model can be applied. A limitation is using classloaders as definition of task.

\subsubsection*{Multitasking Virtual Machine}

Memory accounting is easy because only the objects in old space need to be dynamically accounted. The memory consumption of any task is the sum of size of eden space, from-space and to-space because this amount of memory is reserved to the task. The size of every objects allocated for the task which reside in old space is accounting too. Accounting for consumption in old space is performed in two moments, 1) when a collection in new space move an object from young generation to old generation and 2) when MVM collects old generation.

The overhead of this solution to memory accounting is negligible. Of course this is a direct consequence of design decision in memory layout. However, it is important to note that each isolate in MVM is a single JVM instances. We will have the same problem of lack of resource accounting if we execute a middleware per isolate. Another major problem in MVM is its lack of support for CPU usage accounting.

\subsubsection*{Resource accounting to support resource-aware programming \label{RAJE}}
RAJE is an extension to the Java 2 platform implemented in the context of RASC project. The project's aim is to provide software components with means to express their hardware/software requirements and to use the information at any stage of components life-cycle \cite{guidec:hal-00342142}. The solution considers an application is mapped to a classloader. A resource register is associated with each application and some classes like Memory, CPU, Thread and so on are defined to represent resources. The standard API for Java threads was extended and an application can obtain the amount of resources used by calling some methods. Two models of resources monitoring are used, 1) synchronous monitoring and 2) asynchronous monitoring. The former allows a fine grain resource accounting but it is to expensive and it is not available for all resources. The latter allows accounting resources like CPU usage but it is less accurate.

RAJE was implemented on top of Kaffe for Linux operating system. Some changes were necessary to easy capture CPU usage. Threads subsystem was ported to use native threads so, access to \textbf{/proc} pseudo file-system allows obtaining statistics. Memory accounting is done per-thread by intercepting allocations and collections at virtual machine level.

Although the solution is not portable it offers to applications the ability of observe the resource consumption because every resource object could have any number of attached listeners. Resource objects expose a locking interface with the following semantic: locked resources cannot be used. This mechanism is enough to implement several resource's management policy.

\subsection{Resource-aware programming interface}
Any resource accounting mechanism must provide an API to allow the platform as well as te application to monitor resource usage. In early solutions \cite{czajkowski_jres:_1998, czajkowski_resource_2003}, the center idea was using observer pattern \cite{Gamma:1995:DPE:186897}. In this case, the resource principal assumes the role of \textit{subject} and monitors act as \textit{observers}. Modern solutions rely on Java Management Extensions. In additions to common extensions to control coarse-grained resources like threads, GC cycles and total memory consumption; you can add your own agents to monitor resource consumption to wherever level you want.

In the other side of resource-aware programming, the side related to providing mechanism to adapt the application behavior, there are few options. The most common is provided for the Java runtime itself and it consist in stopping offensive threads or lowering its priority. There are other options which allow using capabilities \cite{Hawblitzel:1998:IMP:1268256.1268278} to revoke permissions. Although there are other solutions \cite{guidec:hal-00342142}, more general adaptation policies are less common and remains a challenges.

\subsection{Discussion}
In addition to resource accounting technique we have described, there are static methods to compute memory consumption and execution time \cite{Puffitsch:2010:WAH:1939345.1939394,Schoeberl:2010:WET:1780354.1780356}. However, these static analysis cannot be applied to an arbitrary application. To apply these methods, developers cannot use certain data structure from class library or they must avoid recursion. In the same way, some fragments of code must be annotated. There are others disadvantages like: lack of support for dynamic loading of code, ignoring garbage collector effect and low accuracy. Moreover, these solutions rely on the construction of Data-Flow Graph and some theoretical can emerge due exponential explosion in the number of state. Although the static analysis is useful for real-time applications, its use is not broadly applicable so, we decided to skip further explanations.

The solutions to resource accounting at user level are too expensive to be considered in production middleware. However, the approach is applicable to part of an application. This can be used to support extensible applications by providing a mechanism to monitor untrusted code. Monitoring specific components within a system can be achieve dynamically or statically. In the former, the JVM machine must be customized because the native code of methods must be changed at runtime. The second solutions may rely on the usual mechanism to deploy components in contemporary middleware. That is, a component is uninstalled, instrumented and then reinstalled.

A considerable advantage of using resource accounting at user level is the easy porting to new platform like Android. In such a case you can write a tool to instrument the code before building DEX files. In the other hand, a solution at virtual machine level involves changing the Dalvik VM. Perhaps, you can build a common infrastructure on top of Vmkit to perform resource accounting and then you can use this infrastructure to build a JVM and a Dalvik VM but, it is a very time consuming task.

Solutions at virtual machine level are easier to implement. Moreover, the performance overhead tend to be slower because the control of resource is direct. However, the solution at virtual machine level is not good for developers of user applications. Perhaps, the best way to deal with resource accounting is by mixing the two approaches. Some resources can be accounted at user level and others to virtual machine level. We can think in offering a low-level API for resource accounting and using such API at user level to aggregate results in wherever structure we consider an independent task.

Finally, it is important to note that there is an obvious lack of solutions for accounting of resources others than memory and CPU usage. The common approach to network usage accounting involves modifying the Java class library. This is not a big modification because all traffic is managed using a few functions. However, there are others network resources like sockets that are not controlled at all. The file system is other resource which is not considered in most publication. It means that resource accounting is still a challenge.

\begin{comment}

\begin{itemize}
\item Resource-aware programming, a broad view. \\
The \textbf{goal} of this section is to introduce and discuss the main concepts regarding resource-aware programming.
In short, I am going to present how important is, in some contexts, developing applications which are able to monitor and manage the resources consumed.
This is specially important in middleware, system with continuous evolution and open environments.
I shall mention the goals we pursuit when we put attention to the resource consumption's concern: QoS, reliability, availability, performance, security, resource-constrained devices.
This section also mentions how this important not only for applications but for applications framework too.

\textbf{Discuss} the following topics that are related to resource management:
\begin{itemize}
\item Resource monitoring
\item Resource reservation
\item Resource isolation
\item Reflection
\end{itemize}

\item Resource monitoring and reservation technologies.\\
Aqui \textbf{discutire} los metodos para hacer monitroing and reservation. Primero algo muy general, pasando a discutir el tema y su importancia in nowadays managed runtime enviroments. La mayoria de este contenido puedo tomarlo desde el reporte.

Los \textbf{objetivos} son: destacar las tecnologias que hay, mencionar sus limitacion. Las limitacion a destacar son: el problema de la portabilidad (muchas implementaciones de la JVM), el problema del performance, el problema de lidear con variadas abstracciones de dominio especifico. 

\item Domain-specific abstractions. \\
En esta seccion se discutira escencialmente que son los domain-specific languages y caules son sus limitaciones en relacion a tooling support cuando se trata de profilers y debuggers. Argumentar esta carencia es facil cuando hablamos de lenguages como Xtend y Kermeta3. Para lenguajes mas especificos como el ubiquo State Machine ya no es tan facil de argumentar porque esas abstracciones no tienen un modelo de ejecucion tan claro o si lo tienen entonces el consumo de memoria no es relevante.

\item Component-based software engineering.\\
Esta seccion \textbf{discute} primero, y solo brevemente, los elementos fundamentales de CBSE, sus ventajas y nivel de adopcion en la practica. Por ultimo menciona que existen varios modelos de componentes y hace una breve recapitulacion de los mismo haciendo enfazis en aquellos que tienen implementacion en managed runtime environments.
Es vital en esta parte hacer una descripcion de los aspectos de estos modelos que los hacen unicos o similares en terminos de control de recursos.
Lo siguiente es relacionar componentes con resource-aware programming. In short, como los modelos de componentes pueden beneficiarse de resource-aware programming para ofrecer un ambiente mas util a los componentes. 

Disutir como el concepto de componente es un tipo especial de abstraccion que presenta para un monitor de consumo de memoria los mismos retos que otras abstracciones. Por lo tanto, debe de intentar generalizarse la definiciones de monitores de consumo de memoria

Me pregunto si debo usar aqui el termino middleware?

Los \textbf{objetivos} de la seccion son: enmarcar la tesis en un contexto de componentes, resaltar la utilidad y desafios de hacer resource-aware programming for component models. Ademas, DEBE decirse what have been done en resource-aware programming y resource mangement para modelos de componentes en el pasado.
Entonces viene la parte dificil: indicar las limitaciones de lo existentes. En mi opinion, no son tanto limitaciones como enfoques diferentes, o quizas la limitacion esta en que el proceso de seleccion del component binding nunca ha estado dirigido por la necesidad de disminuir el overhead de hacer resource management at runtime.

\end{itemize}

\end{comment}