\section{Evaluation}\label{sec:evaluation}
In this section we evaluate the implementation of our approach with several experiments.
%The evaluation is performed in two parts.
To do so, we present a set of experiments to evaluate the performance overhead of executing different memory analysis.
%Afterward, we discuss the expressiveness of our DSL by comparing it against existent solutions.
%In this section we evau the potential benefits of our approach, in particular we want to show how it reduces the overhead in comparison to other generic approaches based on profilers.
%We also want to show the benefits on reduced complexity by showing the number of lines needed to code a monitor of memory consumption with both our approach and plain JVMTI.
A goal of this section is to show that our approach induces small overhead across different applications and types of analysis produced with our approach. Since such analysis exhibit different levels of complexity, we cover a wide spectrum of use even if we are running the experiments with few types of analysis.
We evaluate the performance of our DSL with several experiments:
% Stress clearly that this evaluation is about the analysis performance on different usage
\begin{enumerate}
\item In a first experiment, we assess the overhead on total execution time produced by the periodic computation of the analysis described in listing~\ref{kevoreeaccounting}.
This is a controlled experiment, which uses a macro-benchmark, intended to check how a \textit{generic} analysis behaves on different applications.
In this experiment we measure the overhead of our approach, but we also compare it against the overhead produced by other solutions.
%Test the global time to finish a task while performing a particular analysis many times 

\item In a second experiment, we assess the execution time needed to perform a periodic memory analysis instead of focusing on the total application execution time.
Since the analysis execution time depends on the number of relevant objects in the heap, we vary the input size of the benchmark suite which directly impact the number of objects in memory.
% Assessing the impact of various analysis on the execution performance. This analysis iterate overall the object in memory. We perform the same analysis regularaly on the program but with different input size (meaning different application size with a varying number of objects in memory which impact the analysis performance)

\item In a third experiment, we perform memory analysis on actual applications including \textit{Eclipse}, \textit{NetBean} and others to assess the overhead of the analysis in listing~\ref{topcomponents} on ``real use'' scenarios.
All these applications are built on top of OSGi and this experiment aims at comparing the analysis time of our approach against the time used by well-established memory profilers.  
% Check the performance on real applications (not benchmark) which both show the overhead and highlight that our approach is feasible on real application. Again another analysis to show the expressiveness of the DSL.
\end{enumerate}

In general, all experiments show that the overhead of our DSL is small and has less impact on production applications than well-known memory profilers.

\subsection{Methodology and Setup}\label{sec:MethodologyAndSetup}
Our system is implemented on top of JVMTI, thus we compare our results using the HotSpot Java Virtual Machine version 1.7.0\_76, with a heap size of 2GiB for all the experiments.
Across this section we use Eclipse Memory Analyzer 1.4.0 (Eclipse MAT),  a production ready memory profiler, to perform several experiments.
We use  this tool in console user interface (CLI) mode that executes the desired analysis on a separate process.
In short, when performing a memory analysis on a JVM instance \textit{A}, we dump its heap and invoke Eclipse MAT in a separate JVM instance to collect profiling data.

We use DaCapo benchmarks version 2006-10-MR2~\cite{DaCapo:paper} with the large input size for the first experiment and with different input sizes in the second one.
In the third experiment we use a set of actual applications based on OSGi, these applications are listed in the relevant section~\footnote{Links are available at https://en.wikipedia.org/wiki/OSGi}.
Although the details are specific to each experiment, in general each measurement presented is the average of several runs under the same conditions.

To obtain comparable and reproducible results, we used the same hardware across all experiments: a 2.90GHz Intel(R) i7-3520M processor, running Linux with a 64 bit kernel version 3.17.3 and 8GiB of system memory.

\subsection{Impact of Analysis on the Total Execution Time}

In this experiment we assess the overhead of our approach on the total execution time of applications.
To do so, we compare the execution time reported by the DaCapo benchmarks without any kind of memory analysis against the execution time when our DSL is used to perform the analysis in listing~\ref{kevoreeaccounting}.
In addition, we check how our approach behaves in comparison to other approaches for memory analysis.
In this case, the analysis finds the number of objects and their total size when threads are used as only roots to traverse the graph of live objects.

The experiment was configured as follows: within a JVM instance we wrap the execution of the DaCapo Benchmark.
Each DaCapo test is configured to execute 20 warm-up iterations before the final test execution.
This number of warm-ups is used to guarantee a long enough execution time.
A separate thread periodically performs a \textit{memory consumption monitoring step} every 2 seconds by using one of the methods we want to compare: \textit{No analysis}, \textit{Handwritten JVMTI}, \textit{Our approach}, \textit{Heap Dump + Eclipse MAT}.

In addition to our approach, we implemented a \textit{handwritten JVMTI} agent as well as an Eclipse MAT's  extension to collect the same data.
In the \textit{handwritten JVMTI}, we traverse all the references in the graph of live objects starting on the threads, during this process the JVM is fully halted impacting the total application's execution time.
The \textit{Heap Dump + Eclipse MAT} solution uses the approach described in section~\ref{sec:MethodologyAndSetup}; when an analysis is required, the JVM dumps the heap and executes Eclipse MAT on a separate process in CLI mode.

\begin{figure}[b]
\centering
\begin{tikzpicture}
\begin{axis}[ybar=0pt, legend style={at={(0.72,1)},
every axis legend/.append style={nodes={right}},
anchor=north,legend columns=1, font=\tiny},
ylabel={Overhead (\%)},
y label style={at={(0.06, 0.5)}},
scaled y ticks = false,
      y tick label style={/pgf/number format/fixed,
      /pgf/number format/1000 sep = \thinspace % Optional if you want to replace comma as the 1000 separator 
      },
xtick=data,ymin=0,
width = \columnwidth,
height = 4.2cm,
bar width = 5,
x tick label style={rotate=45,anchor=east, font=\small},
 axis lines*=left, % Don't display the top and right lines
 symbolic x coords={antlr,fop,hsqldb,jython,chart,luindex,xalan,lusearch, pmd, eclipse}
]
\addplot coordinates 
	{(antlr,3.9781514264) (fop,4.605707750) (hsqldb,29.2388250106) (jython,1.3401924419) (chart,2.9126870659) (luindex,7.4126736676)
	(xalan,3.5175679043) (lusearch,2.1071653048) (pmd,2.1071653048) (eclipse,13.2922104461) };
\addplot coordinates 
	{(antlr,4.6792415918) (fop,10.920169369) (hsqldb,33.4078193658) (jython,7.3669103815) (chart,10.0961181121) (luindex,5.8949045922) 
	(xalan,10.6595492114) (lusearch,8.8185623499) (pmd,11.7847827707) (eclipse,15.7219232736)};
\addplot coordinates 
	{(antlr,28.7859273871) (fop,23.7271506764) (hsqldb,46.0448750552) (jython,32.4395399802) (chart,44.6349538836) (luindex,31.9874187461) 
	(xalan,37.7533619117) (lusearch,12.9664891096) (pmd,33.9112866499) (eclipse,32.6863711858)};
\legend{Handwritten JVMTI, Our approach, Heap Dump + Eclipse MAT}
\end{axis}
\end{tikzpicture}
\caption{Overhead on execution time compared to the execution without memory analysis for different tests in the DaCapo Benchmark\label{fig:evaluationTotalTime}}
\end{figure}

In this experiment, we measure the total time needed to complete the 20 warm-up iterations plus the time required to execute the final test.
We repeat this process 10 times for each test in the DaCapo benchmark suite and take the average as final measurement.
First, it is useful to highlight how many time the analysis is performed.
As we mentioned, the analysis runs periodically, so the number of executions depends on the benchmark and the overhead produced by the analysis method.
With our approach, the analysis is executed a minimum of 10 times for \textit{fop} and a maximum of  366 times for \textit{eclipse}.
Figure~\ref{fig:evaluationTotalTime} depicts the overhead in the total execution time of DaCapo tests for different analysis strategies.
The values are shown as the percentage with respect to the baseline which in this case is obtained when \textit{no analysis} is executed.
It is noteworthy that our approach performs close to the handwritten solution.
Moreover, our solution outperforms the \textit{Heap Dump + Eclipse MAT} approach even when the latter is executing mostly on a separate process without halting the JVM during the analysis.

\subsection{Comparing Analysis Time for Simple Assertion}

In the previous section we show the overhead on the total execution time for different analysis mechanisms.
However, these mechanisms are not executed under the same conditions.
For instance, as we mention in section~\ref{sec:implementation} our implementation suspends the execution of the application while it performs the analysis.
On the contrary, the \textit{Heap Dump + Eclipse MAT} approach only suspends the application while dumping the heap, but the analysis is done in a separate process, so it likely runs in parallel.
Therefore, in this experiment we compare only the analysis time using well-known macro-benchmarks.
Since the analysis time depends on the number of objects visited during the computation, we assess in this experiment the behavior of our approach with a memory analysis that must iterate over all objects to complete.
For the same reason, we repeat the analysis with varying input size, which implies different number of objects in memory, to the macro-benchmarks.

\begin{lstlisting}[escapeinside={(*}{*)},caption=Detecting that there is no instance of a given class., label=lst:SimpleAssertion,float=!h, language=DSL]
set_type all: 
	roots (*$\leftarrow$*) #[]
	membership (*$\leftarrow$*) true
	on_inclusion (*$\leftarrow$*) [ fault (*$\leftarrow$*) fault (*$\lor$*) THIS is UnusedClass ]
	fault : bool (*$\leftarrow$*) false
 
instances_for all have_names = "Check class is not used"
\end{lstlisting}

\begin{figure*}[!ht]
 \centering
 \begin{minipage}[t]{0.45\linewidth}
 \centering
\begin{tikzpicture}
\begin{axis}[
ybar=0pt, 
legend style={at={(0.72,1)},
every axis legend/.append style={nodes={right}},
anchor=north,legend columns=1, font=\tiny},
ylabel={Analysis Time (sec)},
y label style={at={(0.06, 0.5)}},
scaled y ticks = false,
      y tick label style={/pgf/number format/fixed,
      /pgf/number format/1000 sep = \thinspace % Optional if you want to replace comma as the 1000 separator 
      },
xtick=data,ymin=0,
width = \columnwidth,
height = 4.2cm,
bar width = 5,
x tick label style={rotate=45,anchor=east, font=\small},
 axis lines*=left, % Don't display the top and right lines
 symbolic x coords={antlr,fop,hsqldb,jython,chart,luindex,xalan,lusearch, pmd, eclipse}
]
\addplot coordinates 
	{(antlr,1.9781514264) (fop,1.605707750) (hsqldb,2.2388250106) (jython,1.3401924419) (chart,2.9126870659) (luindex,1.4126736676)
	(xalan,1.5175679043) (lusearch,2.1071653048) (pmd,1.1071653048) (eclipse,3.2922104461) };
\addplot coordinates 
	{(antlr,2.2781514264) (fop,1.805707750) (hsqldb,2.5388250106) (jython,1.6401924419) (chart,3.2126870659) (luindex,1.6126736676)
		(xalan,1.7175679043) (lusearch,2.4071653048) (pmd,1.3071653048) (eclipse,3.5922104461) };
\addplot coordinates 
	{(antlr,2.9781514264) (fop,2.605707750) (hsqldb,3.2388250106) (jython,2.3401924419) (chart,3.9126870659) (luindex,2.4126736676)
		(xalan,2.5175679043) (lusearch,3.1071653048) (pmd,2.1071653048) (eclipse,4.2922104461) };
%\legend{Handwritten JVMTI, Our approach, Heap Dump + Eclipse MAT}
\end{axis}
\end{tikzpicture}
\caption{Analysis time with default input size\label{fig:analysisTimeDefaultSize}}
\end{minipage}
 \begin{minipage}[t]{0.45\linewidth}
 \centering
\begin{tikzpicture}
\begin{axis}[ybar=0pt, legend style={at={(0.23,1.13)},
every axis legend/.append style={nodes={right}},
anchor=north,legend columns=1, font=\tiny},
ylabel={Analysis Time (sec)},
y label style={at={(0.06, 0.5)}},
scaled y ticks = false,
      y tick label style={/pgf/number format/fixed,
      /pgf/number format/1000 sep = \thinspace % Optional if you want to replace comma as the 1000 separator 
      },
xtick=data,ymin=0,
width = \columnwidth,
height = 4.2cm,
bar width = 5,
x tick label style={rotate=45,anchor=east, font=\small},
 axis lines*=left, % Don't display the top and right lines
 symbolic x coords={antlr,fop,hsqldb,jython,chart,luindex,xalan,lusearch, pmd, eclipse}
]
\addplot coordinates 
	{(antlr,2.3781514264) (fop,1.905707750) (hsqldb,2.7388250106) (jython,1.8401924419) (chart,3.1126870659) (luindex,1.6126736676)
	(xalan,1.7175679043) (lusearch,2.2171653048) (pmd,1.3171653048) (eclipse,3.3822104461) };
\addplot coordinates 
	{(antlr,2.5781514264) (fop,1.945707750) (hsqldb, 2.7818250106) (jython,2.0401924419) (chart,3.6326870659) (luindex,1.912632376)
		(xalan,1.9375679043) (lusearch,2.4071653048) (pmd,1.3999716530) (eclipse,3.7922104461) };
\addplot coordinates 
	{(antlr,3.9781514264) (fop,3.605707750) (hsqldb,4.2388250106) (jython,3.3401924419) (chart,4.9126870659) (luindex,3.4126736676)
		(xalan,3.5175679043) (lusearch,4.1071653048) (pmd,3.1071653048) (eclipse,5.2922104461) };
\legend{Handwritten JVMTI, Our approach, Heap Dump + Eclipse MAT}
\end{axis}
\end{tikzpicture}
\caption{Analysis time with large input size\label{fig:analysisTimeLargeSize}}
 \end{minipage}
\hspace{1cm}
\end{figure*}

Listing~\ref{lst:SimpleAssertion} shows the analysis we execute in this experiment.
It is a simple assertion to check that there does not exist any instance of a given class in the application.
The \textit{set\_type} definition guarantees that all objects are visited by defining the \textit{membership} property as the \textit{true} constant.

The set up of the experiment is as follow.
The DaCapo benchmark suite is used with two different input sizes, default and large.
Before the final test, twenty warm-ups are executed in order to ensure long enough execution time.
A separate thread periodically checks the assertion and records the analysis time.
The average analysis time along the complete execution of a benchmark (i.e., xalan, fop, ...) is used as data point.
Ten of these data points are obtained through repetition of the previous step and used as final measurement for a pair of benchmark and analysis approach.
As in the previous experiment, we use a handwritten JVMTI agents and an Eclipse MAT extension to efficiently check the assertion with those tools.

Figures~\ref{fig:analysisTimeDefaultSize} and~\ref{fig:analysisTimeLargeSize} present the results of the experiments.
In both cases, default and large input size, our DSL is in between the handwritten JVMTI agent and the Eclipse MAT approach.
In comparison to Eclipse MAT, our approach on average reduces the analysis time in 25\% and 39\% for default and large input size respectively.
As expected, the analysis time increases with the number of objects, the slowdown shown between default and large input size is of 8.42\%.

\subsection{Analysis Time in Real Scenarios}
To evaluate the overhead of our approach in actual applications, we execute an analysis to determine the memory consumption that can be related to each Java classloader.
This strategy is common on many Java-based component frameworks because modules' dependencies are often isolated in classloaders; hence, a classloader is a runtime representation related to the component abstraction in such frameworks.
Since OSGi is a widely used framework, we chose several applications build on top of OSGi or supporting it to execute the analysis described in listing~\ref{topcomponents}.
Often, production ready memory profilers include built-in analysis for OSGi bundles or some form of component reports.
For instance, Eclipse MAT provides a report to compute the memory retained for top level classloaders.

This experiment aims at evaluating the analysis time for each application using our approach and \textit{Heap Dump + Eclipse MAT}.
In this experiment, each application is executed until it goes idle, then the memory analysis is executed and its execution time is measured.
This process is repeated ten times for each application and analysis approach in order to use the average as final measurement.
Our DSL computes the analysis in listing~\ref{topcomponents} while \textit{Heap Dump + Eclipse MAT} executes a standard analysis namely \textit{top components} reports.

To execute the memory analysis from within the applications, we implemented extensions for each application. 
For instance, we implemented an Eclipse plugin, a NetBean module and so on.
These extensions are in charge of triggering the analysis.
It was necessary because in our approach the analysis must be executed by the JVM that is being profiled.
In this experiment, we perform the analysis on the following systems: Eclipse Luna~\cite{luna}, NetBeans 8.0\cite{netbeans}, dotCMS 3.1~\cite{dotcms}, Cytoscape 3.2.1~\cite{cytoscape}, Glassfish 4.1~\cite{glassfish},  Liferay 6.2.2~\cite{liferay}, WildFly 8.2~\cite{wildfly}.

Figure~\ref{fig:analysisTime} presents the analysis time for several applications and two analysis approaches. Our DSL outperform other solutions in all applications.
The gain is 3x-19x with an average of 8x.
This gain is due to two factors.
First the Eclipse MAT solution invests some time parsing the dump file and creating the internal indexes to accelerate queries' response time.
Second, the \textit{top components} report in Eclipse MAT can only be implemented using its query language in terms of the function \textit{retainedHeapSize} which calculates the amount of memory retained for a given object.
Since this function is costly to compute, Eclipse MAT spends a considerable amount of the time on it while building the \textit{top components} report. All the evaluation code is available online~\footnote{https://github.com/intigonzalez/heapexplorer\_language}.

\begin{figure}[!b]
\centering
\begin{tikzpicture}
\begin{axis}[ybar=0pt, legend style={at={(0.72,1)},
every axis legend/.append style={nodes={right}},
anchor=north,legend columns=1, font=\tiny},
ylabel={Analysis Time (sec)},
y label style={at={(0.06, 0.5)}},
scaled y ticks = false,
      y tick label style={/pgf/number format/fixed,
      /pgf/number format/1000 sep = \thinspace % Optional if you want to replace comma as the 1000 separator 
      },
xtick=data,ymin=0,
width = \columnwidth,
height = 4.2cm,
bar width = 5,
x tick label style={rotate=45,anchor=east, font=\small},
 axis lines*=left, % Don't display the top and right lines
 symbolic x coords={Eclipse Luna, NetBean 8.0, dotCMS 3.1,Cytoscape 3.2.1,Glassfish 4.1, Liferay 6.2.2, WildFly 8.2}
]
\addplot coordinates 
	{(Eclipse Luna,3.9781514264) (NetBean 8.0, 4.605707750) (dotCMS 3.1, 9.2388250106) (Cytoscape 3.2.1, 1.3401924419) (Glassfish 4.1, 2.9126870659) (Liferay 6.2.2,4.9126870659) (WildFly 8.2, 3.9126870659) };
\addplot coordinates 
	{(Eclipse Luna,42.133233423) (NetBean 8.0,38.388906289) (dotCMS 3.1,30.9167577408) (Cytoscape 3.2.1,25.99) (Glassfish 4.1, 18.46) (Liferay 6.2.2, 28.9126870659) (WildFly 8.2, 19.9126870659)};
\legend{Our Approach, Heap Dump + Eclipse MAT}
\end{axis}
\end{tikzpicture}
\caption{Analysis time for real applications. It shows the time needed to compute an analysis just once. The analysis aims at finding the consumption of the top components as shown in listing~\ref{fig:analysisTime}.\label{fig:analysisTime}}
\end{figure}

%The results shown in figure~\ref{fig:evaluation} confirm the conclusions already discussed.
%Furthermore, they gave an initial estimation of the baseline overhead we can expect when the DSL approach is used.