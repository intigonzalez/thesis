\section{Evaluating performance of profilers}\label{sec:dsl-evaluation}
%\todo{Add research questions}

In this section, we evaluate the implementation of our approach.
To do so, we present experiments to measure the performance overhead induced by memory profilers built using the proposed approach.
This section aims at assessing whether our approach induces low overhead across different applications and types of analysis.
Indeed, using memory profilers that have different levels of complexity, makes our evaluation closer to the expected use in real-world scenarios.

The goal of this section is answering the following research questions:
\begin{enumerate}
\item \textbf{RQ1. Does our approach produce profilers with lower overhead than state-of-the-art tools when used to perform many iterations of memory analysis at runtime?} To answer this question, we assess the overhead on total execution time produced by the periodic computation of a specific analysis.
In this experiment, we measure and compare the overhead of our approach against the overhead produced by other solutions.
\item \textbf{RQ2. Is significant the difference between the time needed to execute a single analysis with our approach in comparison to previous solutions? }
In a second experiment, we measure the execution time needed to perform a single memory analysis step instead of focusing on the total application execution time.
\item \textbf{RQ3. Does the advantage of our approach remain for real applications? }
 Finally, we perform memory analysis on actual applications, including \textit{Eclipse}, \textit{NetBean}, and others, to assess the overhead of profiling in ``real-life'' scenarios. 
\end{enumerate}

In general, these experiments show that our language produces specific profilers with lower overhead for applications running in production environments than well-known memory profilers.

\subsection{Methodology and Setup}\label{sec:MethodologyAndSetup}
Our system is implemented on top of the JVMTI; thus, we compute our results using the HotSpot JVM version 1.7.0\_76, with a heap size of 2GiB for all the experiments.
Across this section, we use Eclipse Memory Analyzer 1.4.0 (Eclipse MAT), a production ready memory profiler, to perform several experiments.
We use this tool in \gls{CLI} mode; this executes the desired analysis in a separate process.
In other words, in performing a memory analysis on a JVM instance \textit{A}, we dump its heap and invoke Eclipse MAT in a separate JVM instance to collect profiling data.

We use DaCapo benchmarks version 2006-10-MR2~\cite{Blackburn:2006:DBJ:1167473.1167488} in the first two experiments, large input size in the first experiment, and default input sizes in the second one.
In the third experiment, we use a set of actual applications based on OSGi, these applications are listed in the relevant section~\footnote{Links are available at \url{https://en.wikipedia.org/wiki/OSGi}}.
Although the details are specific to each experiment, in general, each measurement presented is the average of several runs under the same conditions.

To obtain comparable and reproducible results, we used the same hardware across all experiments: a 2.90GHz Intel(R) i7-3520M processor, running Linux with a 64 bit kernel version 3.17.3 and 8GiB of system memory.

\subsection{Impact of Analysis on the Total Execution Time}

In this experiment, we assess how much our approach affects the execution time of applications.
To do so, we compare the time reported by the execution of DaCapo benchmarks without any kind of memory analysis against the execution time when our language is used to perform the analysis in Listing~\ref{lst:kevoreeaccounting}.
In addition, we check how our approach behaves in comparison to other approaches for memory analysis.
In this case, the profiler finds the number of objects, and their total size, when threads are used as only roots to traverse the graph of live objects.

The experiment was configured as follows: within a JVM instance, we wrap the execution of the DaCapo Benchmark.
Each DaCapo test is configured to execute 20 warm-up iterations before the final test execution.
This number of warm-ups is used to guarantee a long enough execution time.
A separate thread periodically performs a \textit{memory consumption monitoring step} every 2 seconds by using one of the methods we want to compare: 

\begin{description}
\item[No analysis] In this case, we simply execute the DaCapo Benchmarks without any additional task affecting its performance. This is the baseline for the comparison.  

\item[Handwritten JVMTI] In this solution, we traverse all references in the graph of live objects starting on the threads, during this process the JVM is fully halted, impacting the total application's execution time.

\item[Our approach] We use our language to define the profiler in Listing~\ref{lst:kevoreeaccounting}. It is compiled and used at runtime.

\item[Heap Dump + Eclipse MAT] This method uses the approach described in Section \ref{sec:MethodologyAndSetup}; when an analysis is required, the JVM dumps the heap and executes Eclipse MAT in a separate process in CLI mode.
\end{description}

%We then record the total execution time; in other words, the time required for the 20 warm-ups plus the time used in the final test.
%The idea is to check how much the performance is affected by each method.


\begin{figure}[!ht]
\centering
\begin{tikzpicture}
\begin{axis}[ybar=0pt, legend style={at={(0.72,1)},
every axis legend/.append style={nodes={right}},
anchor=north,legend columns=1, font=\tiny},
ylabel={Overhead (\%)},
y label style={at={(0.02, 0.5)}},
scaled y ticks = false,
      y tick label style={/pgf/number format/fixed,
      /pgf/number format/1000 sep = \thinspace % Optional if you want to replace comma as the 1000 separator 
      },
xtick=data,ymin=0,
width = 0.9\columnwidth,
height = 4.2cm,
bar width = 7,
x tick label style={rotate=45,anchor=east},
 axis lines*=left, % Don't display the top and right lines
 symbolic x coords={antlr,fop,hsqldb,jython,chart,luindex,xalan,lusearch, pmd, eclipse}
]
\addplot coordinates 
	{(antlr,3.9781514264) (fop,4.605707750) (hsqldb,29.2388250106) (jython,1.3401924419) (chart,2.9126870659) (luindex,7.4126736676)
	(xalan,3.5175679043) (lusearch,2.1071653048) (pmd,2.1071653048) (eclipse,13.2922104461) };
\addplot coordinates 
	{(antlr,4.6792415918) (fop,10.920169369) (hsqldb,33.4078193658) (jython,7.3669103815) (chart,10.0961181121) (luindex,5.8949045922) 
	(xalan,10.6595492114) (lusearch,8.8185623499) (pmd,11.7847827707) (eclipse,15.7219232736)};
\addplot coordinates 
	{(antlr,28.7859273871) (fop,23.7271506764) (hsqldb,46.0448750552) (jython,32.4395399802) (chart,44.6349538836) (luindex,31.9874187461) 
	(xalan,37.7533619117) (lusearch,12.9664891096) (pmd,33.9112866499) (eclipse,32.6863711858)};
\legend{Handwritten JVMTI, Our approach, Heap Dump + Eclipse MAT}
\end{axis}
\end{tikzpicture}
\caption{Overhead on execution time compared to the execution without memory analysis for different tests in the DaCapo Benchmark\label{fig:evaluationTotalTime}}
\end{figure}

In this experiment, we measure the total time needed to complete the 20 warm-up iterations plus the time required to execute the final test.
The idea is to check how much the performance is affected by each method.
We repeat this process 10 times for each test in the DaCapo Benchmark suite, and take the average as final measurement.

It is useful to discuss how varies the number of times the analysis is performed.
As we mentioned, profilers runs periodically in this set of experiments; thus, the number of invocations to a profiler depends on the benchmark, and the overhead produced by the profiler itself.
For instance, using our approach, the memory analysis is executed a minimum of 10 times in the \textit{fop} benchmark, and a maximum of 366 times in the \textit{eclipse} benchmark.

Figure~\ref{fig:evaluationTotalTime} depicts the overhead in total execution time for different profiling strategies and Dacapo tests.
The values are shown as the percentage with respect to the baseline, which in this case is obtained when \textit{no analysis} is executed.
It is noteworthy that our approach performs close to the handwritten solution.
Moreover, our solution outperforms the \textit{Heap Dump + Eclipse MAT} approach even when the latter is executing mostly on a separate process without halting the JVM during profiling.
The overhead in our approach remains between 4-33\%, and it is 11.93\% in average.

\subsection{Comparing Analysis Time for an Assertion}

In the previous section, we show the performance overhead on total execution time for different profiling mechanisms.
However, these mechanisms are not executed under the same conditions.
For instance, as we mention in Section~\ref{sec:dsl-implementation}, our implementation suspends the execution of the application while it performs the analysis.
On the contrary, the \textit{Heap Dump + Eclipse MAT} approach only suspends the application while dumping the heap, but the analysis is done in a separate process; hence, it likely runs in parallel.
Therefore, in this experiment, we measure only the \textbf{analysis time}, which is the amount of elapsed time from the beginning of analysis to its end.
To perform these experiments, we use again the Dacapo benchmarks.
Since the analysis time depends on the number of objects visited during the computation, in this experiment, we assess the behavior of our approach using a memory profiler that must iterate over all objects to complete.
For the same reason, we repeat the analysis using different input size for each benchmark; this implies that a different number of objects is found in memory.

The \textbf{assertion} used in this experiment checks \textbf{whether an instance of a specific class exists in the heap}.
The following listing shows how to implement such an assertion using our language.
By defining the membership function as the \textit{true} constant, we guarantee that all objects are visited.

\begin{lstlisting}[escapeinside={(*}{*)},
caption={Detecting if there exists an instance of a specific class.}, 
label=lst:SimpleAssertion,
language=DSL2]
create structure foreach e:#["jvm"], using
   constructor
      initialObjects = #Object[]
      exists = false
   membership  true
   updates
      exists = exists or (this is UnusedClass)
\end{lstlisting}

The setting of the experiment is as follow.
The DaCapo benchmark suite is used with two different input sizes, default and large.
Before the final test, twenty warm-ups are executed in order to ensure long enough execution time.
A separate thread periodically checks the assertion and records the analysis time.
The average analysis time along the complete execution of a benchmark (i.e., xalan, fop, ...) is used as data point.
Ten of these data points are obtained through repetition of the previous step and used as final measurement for a pair of benchmark and analysis approach.
As in the previous experiment, we use a handwritten JVMTI agents and an Eclipse MAT extension to check the assertion with those tools.


\begin{figure*}[!ht]
 \centering
 \begin{minipage}[t]{0.45\linewidth}
 \centering
\begin{tikzpicture}
\begin{axis}[
ybar=0pt, 
legend style={at={(0.72,1)},
every axis legend/.append style={nodes={right}},
anchor=north,legend columns=1, font=\tiny},
ylabel={Analysis Time (sec)},
y label style={at={(0.1, 0.5)}},
scaled y ticks = false,
      y tick label style={/pgf/number format/fixed,
      /pgf/number format/1000 sep = \thinspace % Optional if you want to replace comma as the 1000 separator 
      },
xtick=data,ymin=0,
width = \columnwidth,
height = 4.2cm,
bar width = 4,
x tick label style={rotate=45,anchor=east, font=\small},
 axis lines*=left, % Don't display the top and right lines
 symbolic x coords={antlr,fop,hsqldb,jython,chart,luindex,xalan,lusearch, pmd, eclipse}
]
\addplot coordinates 
	{(antlr,1.9781514264) (fop,1.605707750) (hsqldb,2.2388250106) (jython,1.3401924419) (chart,2.9126870659) (luindex,1.4126736676)
	(xalan,1.5175679043) (lusearch,2.1071653048) (pmd,1.1071653048) (eclipse,3.2922104461) };
\addplot coordinates 
	{(antlr,2.2781514264) (fop,1.805707750) (hsqldb,2.5388250106) (jython,1.6401924419) (chart,3.2126870659) (luindex,1.6126736676)
		(xalan,1.7175679043) (lusearch,2.4071653048) (pmd,1.3071653048) (eclipse,3.5922104461) };
\addplot coordinates 
	{(antlr,2.9781514264) (fop,2.605707750) (hsqldb,3.2388250106) (jython,2.3401924419) (chart,3.9126870659) (luindex,2.4126736676)
		(xalan,2.5175679043) (lusearch,3.1071653048) (pmd,2.1071653048) (eclipse,4.2922104461) };
%\legend{Handwritten JVMTI, Our approach, Heap Dump + Eclipse MAT}
\end{axis}
\end{tikzpicture}
\caption{Analysis time with default input size\label{fig:analysisTimeDefaultSize}}
\end{minipage}
\hspace{0.05\linewidth}
\begin{minipage}[t]{0.45\linewidth}
 \centering
\begin{tikzpicture}
\begin{axis}[ybar=0pt, legend style={at={(0.28,1.13)},
every axis legend/.append style={nodes={right}},
anchor=north,legend columns=1, font=\tiny},
ylabel={Analysis Time (sec)},
y label style={at={(0.1, 0.5)}},
scaled y ticks = false,
      y tick label style={/pgf/number format/fixed,
      /pgf/number format/1000 sep = \thinspace % Optional if you want to replace comma as the 1000 separator 
      },
xtick=data,ymin=0,
width = \columnwidth,
height = 4.2cm,
bar width = 4,
x tick label style={rotate=45,anchor=east, font=\small},
 axis lines*=left, % Don't display the top and right lines
 symbolic x coords={antlr,fop,hsqldb,jython,chart,luindex,xalan,lusearch, pmd, eclipse}
]
\addplot coordinates 
	{(antlr,2.3781514264) (fop,1.905707750) (hsqldb,2.7388250106) (jython,1.8401924419) (chart,3.1126870659) (luindex,1.6126736676)
	(xalan,1.7175679043) (lusearch,2.2171653048) (pmd,1.3171653048) (eclipse,3.3822104461) };
\addplot coordinates 
	{(antlr,2.5781514264) (fop,1.945707750) (hsqldb, 2.7818250106) (jython,2.0401924419) (chart,3.6326870659) (luindex,1.912632376)
		(xalan,1.9375679043) (lusearch,2.4071653048) (pmd,1.3999716530) (eclipse,3.7922104461) };
\addplot coordinates 
	{(antlr,3.9781514264) (fop,3.605707750) (hsqldb,4.2388250106) (jython,3.3401924419) (chart,4.9126870659) (luindex,3.4126736676)
		(xalan,3.5175679043) (lusearch,4.1071653048) (pmd,3.1071653048) (eclipse,5.2922104461) };
\legend{Handwritten JVMTI, Our approach, Heap Dump + Eclipse MAT}
\end{axis}
\end{tikzpicture}
\caption{Analysis time with large input size\label{fig:analysisTimeLargeSize}}
 \end{minipage}
\hspace{1cm}
\end{figure*}

Figures~\ref{fig:analysisTimeDefaultSize} and~\ref{fig:analysisTimeLargeSize} present the results of the experiments.
In both cases, default and large input size, our approach is in between the handwritten JVMTI agent and the Eclipse MAT approach.
In comparison to Eclipse MAT, our approach reduces the analysis time by 25\% and 39\% for default and large input size respectively.
As expected, the analysis time increases with the number of objects, the slowdown shown between default and large input size is of 8.42\%.

\subsection{Profiling Time in Real Scenarios}
To evaluate the overhead of our approach in actual applications, 
we compute the memory consumption of bundles in real OSGi-based systems.
Since OSGi is a widely used framework, we chose applications built on top of OSGi or supporting it.
The custom profiler definition is based on the idea that bundle consumption is the consumption of a Java classloader.
Such a strategy is common when measuring memory consumption for Java-based component frameworks because modules are often isolated and represented through classloaders.
The complete profiler's definition is shown below:
\begin{lstlisting}[escapeinside={(*}{*)},
caption={Calculating the consumption of top components.},
label=topcomponents,
language=DSL2]
create structure foreach e:classloaders using
  constructor
    initialObjects = #[e]
    size = 0
  membership  ((ref_kind == root and this.class.classloader in this_structure) or
	(ref_kind != root and referrer in this_structure))
  updates
    size = size + this.size
\end{lstlisting}

This experiment aims at evaluating the profiling time for each application using our approach and \textit{Heap Dump + Eclipse MAT}.
In this experiment, each application is executed, once it is initialized, the memory profiler is invoked, and its execution time measured.
This process is repeated ten times for each application and analysis approach in order to use the average as final measurement.
We use \textit{Heap Dump + Eclipse MAT}  to compute the memory retained for top level classloaders using  a standard analysis named \textit{top components} reports. 

To execute the memory analysis from within the applications, we implemented extensions for each application (e.g., an Eclipse plugin, a NetBean module). ~\footnote{ The evaluation code is available online: \url{https://github.com/intigonzalez/heapexplorer\_language}}.

These extensions are in charge of triggering the analysis.
It was necessary because in our approach the analysis must be executed by the JVM that is being profiled.
In this experiment, we perform the analysis on the following systems: Eclipse Luna~\cite{luna}, NetBeans 8.0\cite{netbeans}, dotCMS 3.1~\cite{dotcms}, Cytoscape 3.2.1~\cite{cytoscape}, Glassfish 4.1~\cite{glassfish},  Liferay 6.2.2~\cite{liferay}, and WildFly 8.2~\cite{wildfly}.

\begin{figure}[!h]
\centering
\begin{tikzpicture}
\begin{axis}[ybar=0pt, legend style={at={(0.72,1)},
every axis legend/.append style={nodes={right}},
anchor=north,legend columns=1, font=\tiny},
ylabel={Analysis Time (sec)},
y label style={at={(0.02, 0.5)}},
scaled y ticks = false,
      y tick label style={/pgf/number format/fixed,
      /pgf/number format/1000 sep = \thinspace % Optional if you want to replace comma as the 1000 separator 
      },
xtick=data,ymin=0,
width = 0.8\columnwidth,
height = 4.2cm,
bar width = 7,
x tick label style={rotate=45,anchor=east, font=\small},
 axis lines*=left, % Don't display the top and right lines
 symbolic x coords={Eclipse Luna, NetBean 8.0, dotCMS 3.1,Cytoscape 3.2.1,Glassfish 4.1, Liferay 6.2.2, WildFly 8.2}
]
\addplot coordinates 
	{(Eclipse Luna,3.9781514264) (NetBean 8.0, 4.605707750) (dotCMS 3.1, 9.2388250106) (Cytoscape 3.2.1, 1.3401924419) (Glassfish 4.1, 2.9126870659) (Liferay 6.2.2,4.9126870659) (WildFly 8.2, 3.9126870659) };
\addplot coordinates 
	{(Eclipse Luna,42.133233423) (NetBean 8.0,38.388906289) (dotCMS 3.1,30.9167577408) (Cytoscape 3.2.1,25.99) (Glassfish 4.1, 18.46) (Liferay 6.2.2, 28.9126870659) (WildFly 8.2, 19.9126870659)};
\legend{Our Approach, Heap Dump + Eclipse MAT}
\end{axis}
\end{tikzpicture}
\caption{Analysis time for real applications. It shows the time needed to compute an analysis just once. The analysis aims at finding the consumption of the top components\label{fig:analysisTime}}
\end{figure}

Figure~\ref{fig:analysisTime} presents the analysis time for several applications and two analysis approaches.
Our approach outperforms Eclipse MAT in all applications; the gain is 3x-19x with an average of 8x.
Two factors influence the measurements.
First , Eclipse MAT invests some time parsing the dump file, and creating the internal indexes to accelerate queries' response time.
Second, the \textit{top components} report in Eclipse MAT can only be implemented, using its query language, in terms of the function \textit{retainedHeapSize}, which calculates the amount of memory retained for a given object.
Since this function is costly to compute, Eclipse MAT spends a considerable amount of time on it while building the \textit{top components} report.

%The results shown in figure~\ref{fig:evaluation} confirm the conclusions already discussed.
%Furthermore, they gave an initial estimation of the baseline overhead we can expect when the DSL approach is used.