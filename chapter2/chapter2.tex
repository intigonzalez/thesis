%------------------------------%
\selectlanguage{english}
\chapter{Abstraction-oriented resource awareness}
\label{chap:abstractions_and_resource_management}
\markboth{Abstraction-oriented resource awareness}{Chapter2}
%------------------------------%

\coolphrase {Hey, look at this}{Inti Gonzalez-Herrera}

Defining and using abstractions is a main part of building software products.
New abstractions are usually layered atop existent software systems in order to fulfill specific goals
- ease development, improve security, provide extensibility.
Often, these abstractions can be seen as isolated software entities that consume computational resources (e.g., process, thread).
Since abstractions are the building block of applications, it is of utmost importance to control their behavior regarding resource usage.
  
This chapter describes two abstractions that are frequently used on top of MRTEs: components (section \ref{sec:components-oriented-resource-awareness}), and domain-specific languages (DSLs) (section \ref{sec:DSL-on-MRTEs})
Comprehensive discussions, on how these abstractions consume resources, are presented.
In particular, we are interested on two phenomenons.
First, we tackle (section \ref{sec:component-leverage}) the issue of how we can leverage component-based architectures, to reduce the overhead of dealing with resources, by specializing resource management techniques.

Second, since new abstractions are constantly created nowadays, having generic and efficient mechanisms for controlling how they use resources may simplify the process of providing resource awareness support.
We then present the state of the art on supporting some forms of resource accounting for arbitrary abstractions (section \ref{sec:resource-awareness-for-dsl}).

\section{Developer's View versus Runtime's View} \label{sec:chapter2-introduction}

As mentioned, building abstractions is at the core of software development.
They are meant to tackle a large number of problems in software engineering, ranging from providing better representation of the business logic to supporting application's extensibility.
Interestingly, abstractions are not built from scratch; instead, they are implemented upon other abstractions provided by the runtime environment.
This leads to the well-known layered architecture where complex features are created using more simple concepts.
In th field of operating systems, processes are built relying on low-level concepts such as hardware interrupts, context-switch, and MMU hardware.
In the area of programming languages, recursive routines are implemented upon basic hardware stack manipulation.

The beauty of abstractions lies on the fact that they are meant to be blindly used.
In other words, once a new abstraction is implemented and its invariants defined, you can use it without a complete understanding of the implementation details.
Nevertheless, this has profound implications on the software development process because, from the moment a new abstraction is used, developers start thinking in abstraction's terms.
For instance, showing only plain assembler instructions while debugging applications is no longer good choice once you start coding your applications in a language that includes high-level concepts such as routines, loops, and conditional-statements.
In the same way, when a profiler is used to check the memory consumption of Java-based applications, the data produced is expected to reflect terms such as \textit{object} and \textit{class}.
To summarize, tools such as editors, debuggers, and profilers must be updated to make them aware of each new abstraction introduced in the development cycle.
In this thesis we say that a mismatch exists, between the developer's view and the tooling's view, when the concepts managed by the developers are not clearly reflected in the tools.

In using MRTEs to develop and run applications, this kind of mismatch often occurs.
A couple of illustrative examples are give below.

Usually, many OSGi bundles are deployed on top of a single JVM instance.
Due to the communication mechanism used in OSGi, where objects are routinely shared, it is complex to decide which bundle should be accounted for the consumption of a particular object.
A possible approach is deciding that an object $O$ is being consumed by a bundle if $O$'s class was loaded using the classloader $C$ associated with such a bundle.
Then, we can use this mechanism to monitor per-bundle memory consumption. 
However, a profiler must perform considerable amount of processing to collect such kind of data because it is not straightforwardly available in the JVM. 
%Since we know how to represent a bundle using Java concepts, this solution is feasible.
Given the widespread usage of OSGi, memory profilers often support collecting data regarding per-bundle memory usage.
Unfortunately, similar abstractions (components models), equally implemented atop of Java, are often not properly supported by such tools because they are not as popular as OSGi.
Hence, data must be manually aggregated when an application using a poorly supported abstraction is profiled; this is, of course, a considerable burden for developers.

Many of the newly designed DSLs are built on top of existing object-oriented languages runtime such as the JVM. 
Therefore, people in charge of optimizing, debugging and maintaining software applications can use the existing debugger and profilers of these platforms. 
However, there is clear mismatch between classical profilers used in object-oriented systems and the newly designed languages. 
Indeed, the concepts introduced in these new DSLs may not exhibit a straightforward mapping to the underlying object-oriented system.
As a consequence, it may be time consuming and complex to use a classical profiler to check applications that are based on these new languages.

For instance, active annotations allow developers to participate in the translation process of Xtend source code to Java code via library.
Such mechanism is often used directly by developers to introduce their own abstraction or defining their own internal DSL. 
For example, in the K3-AL\footnote{Available at https://github.com/diverse-project/k3/wiki} project, we use active annotations to create an open-class mechanism on top of Java \cite{Clifton:2000:MMO:353171.353181}. 
As a result, the annotation processor change the program structure to implement this feature. 
It adds some methods indirection (to $AASpect$), create new set of objects that represents the state of one conceptual object ($AASpectProperty$), use the Xtend extension method feature, etc\dots.
Figure~\ref{fig:k3-diagram} shows the result of this translation process. 
The left part illustrates the code written by the developer, the middle part shows the developer view (the code that can be written to use the open-class mechanism), the right part describes the runtime view.
In this case, an instance of an open-class is not represented by a single JVM object; instead, it is represented by several scatter objects.  

\begin{figure*}
\centering
\includegraphics[width=0.9\linewidth]{chapter2/fig/famous}
\caption{Translation process used for building developer abstraction using annotations}
\label{fig:k3-diagram}
\end{figure*}

\section{Component-oriented Resource Awareness} \label{sec:components-oriented-resource-awareness}

Software architecture aims at reducing complexity through abstraction and separation of concerns by providing a common understanding of component, connector and configuration~\cite{xadl,Medvidovic:2000,VanOmmering-et-al-00}.
One of the benefits is that it facilitates the management of dynamic architectures, which becomes a primary concern in the Future Internet and Cyber-Physical Systems~\cite{DBLP:journals/ase/NittoGMPP08, Johnson:2015:CSM:2735960.2735979}.
Such systems demand techniques that let software react to changes by self-organizing its structure and self-adapting its behavior~\cite{PanzicaLaManna:2012:LDU:2304736.2304764, Johnson:2015:CSM:2735960.2735979, Zhang:2009:MVD:1509239.1509262}.
Many works~\cite{cbse-conference} have shown the benefits of using component-based approaches in such open-world environments~\cite{baresi2006toward, Caporuscio:2010:AIA:1985522.1985547, Perez-Palacin:2010:PAO:1712605.1712614}.

To satisfy the needs for adaptation, several component models provide solutions to dynamically reconfigure a software architecture through, for example, the deployment of new modules, the instantiation of new services, and the creation of new bindings between components~\cite{Porter:2014:RMC:2602458.2602471, Zheng:2014:RCC:2679601.2680405, Irmert:2008:RAS:1370018.1370036, Ghezzi:2010:QDD:2163764.2163774}. 
In practice, component-based (and/or service-based) platforms like Fractal~\cite{bruneton06}, OpenCOM~\cite{BlairCULJ04}, OSGi~\cite{OSGI:r5} or SCA~\cite{SEINTURIER:2011:INRIA-00567442:1} provide platform mechanisms to support dynamic architectures.


\begin{quote}
\textit{[\dots] software components are executable units of independent production, 
acquisition, and deployment that can be composed into a functioning system.
To enable composition, a software component adheres to a particular component
model and targets a particular component platform.
}
\end{quote}

\subsection{Component Models}
\subsubsection{EJB}
\subsubsection{Fractal}
This paper presents FRACTAL \cite{Bruneton:2006:FCM:1152333.1152345}, a hierarchical and reflective component model with sharing.
Components in this model can be endowed with arbitrary reflective capabilities, from plain black-box objects to
components that allow a fine-grained manipulation of their internal structure.
The paper describes JULIA, a Java implementation of the model, a small but efficient runtime framework, which relies on
a combination of interceptors and mixins for the programming of reflective features of components.
The paper presents a qualitative and quantitative evaluation of this implementation, showing that
component-based programming in FRACTAL can be made very efficient. 
\subsubsection{OSGi}
\subsubsection{ROBOcop}
\subsubsection{Kevoree}

Built on top of dynamic component frameworks, Models@run.time denote model-driven approaches that aim at taming the complexity of dynamic adaptation.
It basically pushes the idea of reflection~\cite{morin09a} one step further by considering the reflection-layer as a real model: ``something simpler, safer or cheaper than reality to avoid the complexity, danger and irreversibility of reality''.
In practice, component-based and service-based platforms offer reflection APIs that allow instrospecting the application (e.g., which components and bindings are currently in place in the system) and dynamic adaptation (e.g., changing the current components and bindings).
While some of these platforms offer rollback mechanisms to recover after an erroneous adaptation~\cite{leger2010reliable}, the purpose of Models@run.time is to prevent the system from actually enacting an erroneous adaptation. 
In other words, the ``model at runtime'' is a reflection model that can be decoupled from the application (for reasoning, validation, and simulation purposes) and then automatically resynchronized.
This model can not only manage the application's structural information (i.e., the architecture), but can also be populated with behavioural information from the specification or the runtime monitoring data.

Kevoree provides multiple concepts that are used to create a distributed application that allows dynamic adaptation. The \emph{Node} concept is used to model the infrastructure topology and the \emph{Group} concept is used to model the semantics of inter-node communication, particularly when synchronizing the reflection model among nodes. 
Kevoree includes a \emph{Channel} concept to allow for different communication semantics between remote \emph{Components} deployed on heterogeneous nodes. 
All Kevoree concepts (\textit{Component}, \textit{Channel}, \textit{Node}, \textit{Group}) obey the object type design pattern~\cite{johnson_type_1997} in order to separate deployment artifacts from running artifacts.  

%Platforms
Kevoree supports multiple execution platforms (e.g.,~Java, Android, MiniCloud, FreeBSD, Arduino). For each target platform it provides a specific runtime container. 
%Tools
Moreover, Kevoree comes with a set of tools for building dynamic applications (a graphical editor to visualize and edit configurations, a textual language to express reconfigurations, several checkers to valid configurations). 

As a result, Kevoree provides a promising environment by facilitating the implementation of dynamically reconfigurable applications in the context of an open-world environment.
Because our goal is to design and implement an adaptive monitoring system, the introspection and the dynamic reconfiguration facilities offered by Kevoree suit the needs of the ScapeGoat framework.


\subsection{Extra-functional Properties}
\begin{itemize}
\item Describe las diferentes categorias mencionadas por Ivica.
\item Describe el problema de conteo indirecto contra directo.
\item Describe la cuestion del contrato
\end{itemize}
\section{State of the art of resource management for components} \label{sec:component-leverage}
\begin{itemize}
\item Describe las soluciones de kouther
\item ver que mas se puede describir
\end{itemize}

\section{Domain-Specific Languages and MRTEs} \label{sec:DSL-on-MRTEs}

\section{Resource Awareness for Domain-specific Abstractions} \label{sec:resource-awareness-for-dsl}

\subsection{Languages to deal with resource aware}

\paragraph{DiSL: a domain-specific language for bytecode instrumentation, \cite{Marek:2012:DEL:2162037.2162046,Marek2012}}
Dynamic program analysis tools support numerous software engineering tasks, including profiling, debugging, and re-
verse engineering.
Prevailing techniques for building dynamic analysis tools are based on low-level abstractions
that make tool development tedious, error-prone, and expensive. To simplify the development of dynamic analysis tools,
some researchers promoted the use of aspect-oriented programming (AOP). However, as mainstream AOP languages
have not been designed to meet the requirements of dynamic analysis, the success of using AOP in this context remains
limited. For example, in AspectJ, join points that are important for dynamic program analysis (e.g., the execution of
bytecodes or basic blocks of code) are missing, access to reflective dynamic join point information is expensive, data
passing between woven advice in local variables is not supported, and the mixing of low-level bytecode instrumentation
and high-level AOP code is not foreseen. In this talk, we present DiSL [1], a new domain-specific aspect language for
bytecode instrumentation.
DiSL uses Java annotation syntax such that standard Java compilers can be used for compiling
DiSL code. The language features an open join point model, novel constructs inspired by weave-time evaluation of
conditional join points and by staged execution, and access to custom static and dynamic context information.
Moreover, the DiSL weaver guarantees complete bytecode coverage.
We have implemented several dynamic analysis tools in DiSL,
including profilers for the inter- and intra-procedural control
flow, debuggers, dynamic metrics collectors integrated in the
Eclipse IDE to augment the static source views with dynamic information, and tools for workload characterization.
These tools are concise and perform equally well as implementations using low-level techniques. DiSL has also been
conceived as an intermediate language for future domain-specific analysis languages, as well as for AOP languages.

\paragraph{Javana: A System for Building Customized Java Program Analysis Tools, \cite{Maebe06javana:a}}
Understanding the behavior of applications running on high-level
language virtual machines, as is the case in Java, is non-trivial because of the tight entanglement at the lowest execution level between the application and the virtual machine.

This paper proposes Javana, a system for building Java program analysis tools.
Javana provides an easy-to-use instrumentation infrastructure that allows
for building customized profiling tools very quickly.
Javana runs a dynamic binary instrumentation tool underneath
the virtual machine. The virtual machine communicates with the
instrumentation layer through an event handling mechanism for
building a vertical map that links low-level native instruction pointers and memory addresses to high-level language concepts such as
objects, methods, threads, lines of code, etc.
The dynamic binary instrumentation tool then intercepts all memory accesses and instructions executed and provides the Javana end user with high-
level language information for all memory accesses and natively
executed instructions.

We demonstrate the power of Javana through a number of applications: memory address tracing, vertical cache simulation and
object lifetime computation.
For each of these applications, the instrumentation specification requires only a small number of lines
of code.
Developing similarly powerful profiling tools within a virtual machine (as done in current practice) is both time-consuming
and error-prone; in addition, the accuracy of the obtained profiling
results might be questionable as we show in this paper.
\paragraph{Flexible and efficient profiling with aspect-oriented programming, \cite{Binder:2006:FEM:1173706.1173733}}
Many profilers for virtual execution environments, such as the Java virtual machine (JVM), are implemented with low-level bytecode instrumentation techniques, which is tedious, error-prone, and complicates maintenance and extension of the tools.
In order to reduce the development time and cost, we promote building profilers for the JVM using high-level aspect-oriented programming (AOP).
We show that the use of aspects yields concise profilers that are easy to develop, extend, and maintain, because low-level
instrumentation details are hidden from the tool developer.
In order to build efficient profilers, we introduce inter-advice communication, an extension to common AOP languages that enables efficient data passing
between advices that are woven into the same method using local variables.
We illustrate our approach with two case studies.
First, we show that an existing, instrumentation-based tool for listener latency
profiling can be easily recast as an aspect.
Second, we present an aspect for comprehensive calling context profiling.
In order to reduce profiling overhead, our aspect parallelizes application execution and profile
creation, resulting in a speedup of 110\% on a machine with more than two cores, compared with a primitive, non-parallel approach
\paragraph{A portable and customizable profiling framework for Java based on bytecode instruction counting, \cite{Binder2005}}
Prevailing profilers for Java, which rely on standard, native-code profiling interfaces, are not portable, give imprecise results due to serious measurement perturbation, and cause excessive overheads.
In contrast, program transformations allow to generate reproducible profiles in a fully portable way with significantly less overhead.
This paper presents a profiling framework that instruments Java programs at the bytecode level to build context-sensitive execution profiles at runtime.
The profiling framework includes an exact profiler as well as a sampling profiler.
User-defined profiling agents can be written in pure Java, too, in order to customize the runtime processing of profiling data.
\paragraph{Profiling with AspectJ, \cite{Pearce:2007:PA:1248445.1248448}}
This paper investigates whether AspectJ can be used for efficient profiling of Java programs.
Profiling differs from other applications of AOP (e.g. tracing), since it necessitates efficient and often complex
interactions with the target program.
As such, it was uncertain whether AspectJ could achieve this goal.
Therefore, we investigate four common profiling problems (heap usage, object lifetime, wasted time and
time-spent) and report on how well AspectJ handles them.
For each, we provide an efficient implementation,
discuss any trade-offs or limitations and present the results of an experimental evaluation into the costs
of using it.
Our conclusions are mixed. On the one hand, we find that AspectJ is sufficiently expressive
to describe the four profiling problems and reasonably efficient in most cases.
On the other hand, we find several limitations with the current AspectJ implementation that severely hamper its suitability for
profiling.

\paragraph{Controlled dynamic performance analysis, \cite{Reiss:2008:CDP:1383559.1383566}}
We are interested in obtaining detailed performance information
on-the-fly from long-running systems without adversely affecting
the performance of the systems.
We have developed a methodology consisting of a framework, DYPER, and a number of
specialized agents called proflets each of which analyzes a
different performance aspect.
DYPER gathers performance information with a guaranteed maximum overhead that is
dynamically settable by the programmer using priorities set by the
proflets.
Moreover, the type of information that the system can
provide is generally only available for tools that generally have too
much overhead to be usable in production or long-running
systems.
DYPER includes the ability to control and display
performance data as the program is run.

\paragraph{A meta-aspect protocol for developing dynamic analyses, \cite{Achenbach:2010:MPD:1939399.1939415} }
Dynamic aspect-oriented programming has been widely used
for the development of dynamic analyses to abstract over low-level program instrumentation.
Due to particular feature requirements in different analysis domains like debugging or testing, many different aspect
languages were developed from scratch or by extensive compiler or interpreter extensions.
We introduce another level of abstraction in form of a meta-aspect protocol to separate the host language from the analysis
domain.
A language expert can use this protocol to tailor an analysis-specific aspect language, based on which a domain expert can develop
a particular analysis.
Our design enables a flexible specification of the join point model, configurability of aspect deployment and scoping, and
extensibility of pointcut and advice language.
We present the application of our design to different dynamic analysis domains.
\paragraph{Comprehensive Profiling Support in the Java Virtual Machine. \cite{Liang1999}}
Existing profilers for Java applications typically rely on custom instrumentation in the Java virtual machine, and measure only limited types of resource consumption. Garbage collection and multi-threading pose additional challenges to profiler design and implementation.

In this paper we discuss a general-purpose, portable, and extensible approach for obtaining comprehensive profiling information from the Java virtual machine. Profilers based on this framework can uncover CPU usage hot spots, heavy memory allocation sites, unnecessary object retention, contended monitors, and thread deadlocks. In addition, we discuss a novel algorithm for thread-aware statistical CPU time profiling, a heap profiling technique independent of the garbage collection implementation, and support for interactive profiling with minimum overhead. 
\paragraph{Profiling Field Initialisation in Java, \cite{Nelson2013}}
\todo{No cre que sea muy bueno}
Java encourages programmers to use constructor methods to
initialise objects, supports final modifiers for documenting fields which
are never modified and employs static checking to ensure such fields
are only ever initialised inside constructors. Unkel and Lam observed
that relatively few fields are actually declared final and showed using
static analysis that many more fields have final behaviour, and even more
fields are stationary (i.e. all writes occur before all reads). We present
results from a runtime analysis of 14 real-world Java programs which
not only replicates Unkel and Lamâ€™s results, but suggests their analysis
may have under-approximated the true figure. Our results indicate a
remarkable 72-82% of fields are stationary, that final is poorly utilised by
Java programmers, and that initialisation of immutable fields frequently
occurs after constructor return. This suggests that the final modifier for
fields does a poor job of supporting common programming practices.
\paragraph{A dynamic optimization framework for a Java just-in-time compiler, \cite{Suganuma:2001:DOF:504311.504296}}
The high performance implementation of Java Virtual Machines (JVM) and just-in-time (JIT) compilers is directed toward adaptive compilation optimizations on the basis of online runtime profile information. This paper describes the design and implementation of a dynamic optimization framework in a production-level Java JIT compiler. Our approach is to employ a mixed mode interpreter and a three level optimizing compiler, supporting quick, full, and special optimization, each of which has a different set of tradeoffs between compilation overhead and execution speed. a lightweight sampling profiler operates continuously during the entire program's exectuion. When necessary, detailed information on runtime behavior is collected by dynmiacally generating instrumentation code which can be installed to and uninstalled from the specified recompilation target code. Value profiling with this instrumentation mechanism allows fully automatic code specialization to be performed on the basis of specific parameter values or global data at the highest optimization level. The experimental results show that our approach offers high performance and a low code expansion ratio in both program startup and steady state measurements in comparison to the compile-only approach, and that the code specialization can also contribute modest performance improvement.

\paragraph{Complete and Platform-Independent Calling Context Profiling for the Java Virtual Machine, \cite{Sarimbekov201161}}
Calling context profiling collects statistics separately for each calling context. Complete calling context profiles that faithfully represent overall program execution are important for a sound analysis of program behavior, which in turn is important for program understanding, reverse engineering, and workload characterization. Many existing calling context profilers for Java rely on sampling or on incomplete instrumentation techniques, yielding incomplete profiles; others rely on Java Virtual Machine (JVM) modifications or work only with one specific JVM, thus compromising portability. In this paper we present a new calling context profiler for Java that reconciles completeness of the collected profiles and full compatibility with any standard JVM. In order to reduce measurement perturbation, our profiler collects platform-independent dynamic metrics, such as the number of method invocations and the number of executed bytecodes. In contrast to prevailing calling context profilers, our tool is able to distinguish between multiple call sites in a method and supports selective profiling of (the dynamic extent of) certain methods. We have evaluate the overhead introduced by our profiler with standard Java and Scala benchmarks on a range of different JVMs.

\subsection{Query languages about memory management}

In DeAl~\cite{Reichenbach:2010:GCE:1869459.1869482}, the authors propose a language to compute heap assertions at garbage collection time.
The language design is motivated by concerns of efficiency as our approach.
There is however a large number of differences, first DeAl is only able to compute boolean outputs while our DSL is intended to produce arbitrary data type as output which also includes boolean values.
DeAl is a purely declarative language while our DSL contains a much more complex execution model.
In exchange for the declarative style and the focus on assertions, DeAl is able to guarantee formal properties about the computation that we cannot provide. 

In~\cite{Xu:2013:PML:2491509.2491511} the authors propose a framework to detect memory leaks associated with Java containers.
To do so, the framework explores the status of each container object with the goal of identifying leak patterns. The approach is based on finding objects which are keeping references to removed bundles. LeakBots~\cite{Mitchell03leakbot:an} is an automated tool to detect memory leaks. It makes heavy
use of information recollected from the heap in order to identify the more likely structures leading to a leak.
These tools collect data from the heap in order to automatically pinpoint a particular memory issue.
The methods to collect the data are handwritten for efficiency reasons.
Lots of data collected in these works is also available with our approach.



\section{Conclusions}

Discutir aqui que problemas nuevos surgen cuando se quiere hacer resource management en component-based systems. Por ejemplo, a quien se le asigna el consumo.

By enforcing a strict separation between interface and implementation and by making software
architecture explicit, component-based programming can facilitate the implementation and
maintenance of complex software systems [1]. Indeed, these two principles form the basis for two
essential properties: adaptability and manageability. Their role as units of software deployment and
configuration in particular, are well understood: they allow for pre-runtime adaptation in order to
suit arbitrary deployment environments (construction of dedicated software infrastructures), evolution
in requirements and technical evolution (maintenance), and organizational evolution (integration,
interoperation). When seen as runtime structures, components can serve as the basis for software
reconfiguration. By fully delineating subsystem boundaries, they provide a natural scope for
reconfiguration actions and a natural target for system instrumentation and supervision. Coupled with
the use of meta-programming techniques, component-based programming can hide from application
programmers some of the complexity inherent in the handling of non-functional aspects in a software
system, such as distribution and fault tolerance, as exemplified by the container concept in Enterprise
Java Beans (EJB), CORBA Component Model (CCM), or Microsoft .Net

It is likely that components of different qualities (level of performance, resource efficiency,
robustness, degree of certification, and so on) will be available at different
prices.

